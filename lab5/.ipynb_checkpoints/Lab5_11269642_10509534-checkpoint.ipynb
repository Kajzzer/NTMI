{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lotte Bottema 11269642\n",
    "## Kaj Meijer 10509534\n",
    "\n",
    "**Table of contents**\n",
    "\n",
    "* [Task](#task)\n",
    "* [Naive Bayes](#NB)\n",
    "    * [Feature function](#ff)\n",
    "    * [MLE](#MLE)\n",
    "    * [Evaluation](#eval)\n",
    "\n",
    "    \n",
    "**Table of Exercises**\n",
    "\n",
    "\n",
    "* [Exercise 5-1](#ex5-1) (-/1)\n",
    "* [Exercise 5-2](#ex5-2) (-/2)\n",
    "* [Exercise 5-3](#ex5-3) (-/3)\n",
    "* [Exercise 5-4](#ex5-4) (-/2)\n",
    "* [Exercise 5-5](#ex5-5) (-/4)\n",
    "\n",
    "\n",
    "\n",
    "**General notes**\n",
    "\n",
    "* In this notebook you are expected to use $\\LaTeX$. \n",
    "* Use python3.\n",
    "* Use NLTK to read annotated data.\n",
    "* **Document your code**: TAs are more likely to understand the steps if you document them. If you don't, it's also difficult to give you partial points for exercises that are not completely correct.\n",
    "\n",
    "After completing this lab you should be able to \n",
    "\n",
    "* develop Naive Bayes text classifiers\n",
    "* estimate parameters via MLE\n",
    "* predict and evaluate models using precision/recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"task\"> Task\n",
    "\n",
    "We will be looking into binary sentiment analysis where we have to decide whether a document $x$ (a list of tokens) is positive (class $y=1$) or negative (class $y=0$) towards a subject.\n",
    "\n",
    "The dataset we will use comes from NLTK [nltk.corpus.sentence_polarity](http://www.nltk.org/howto/corpus.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import sentence_polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains 5331 positive and 5331 negative sentences, which you can obtain as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5331 positive sentences such as:\n",
      " the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "5331 negative sentences such as:\n",
      " simplistic , silly and tedious .\n"
     ]
    }
   ],
   "source": [
    "pos_sents = sentence_polarity.sents(categories='pos')\n",
    "neg_sents = sentence_polarity.sents(categories='neg')\n",
    "print(len(pos_sents), 'positive sentences such as:\\n', ' '.join(pos_sents[0]))\n",
    "print(len(neg_sents), 'negative sentences such as:\\n', ' '.join(neg_sents[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the first 4000 sentences from each class for training, the next 331 for development, and the last 1000 for test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 4000 pos and 4000 neg\n",
      "Development: 331 pos and 331 neg\n",
      "Test: 1000 pos and 1000 neg\n"
     ]
    }
   ],
   "source": [
    "training_pos = pos_sents[:4000]\n",
    "training_neg = neg_sents[:4000]\n",
    "dev_pos = pos_sents[4000:4331]\n",
    "dev_neg = neg_sents[4000:4331]\n",
    "test_pos = pos_sents[4331:]\n",
    "test_neg = neg_sents[4331:]\n",
    "print('Training: %d pos and %d neg' % (len(training_pos), len(training_neg)))\n",
    "print('Development: %d pos and %d neg' % (len(dev_pos), len(dev_neg)))\n",
    "print('Test: %d pos and %d neg' % (len(test_pos), len(test_neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_docs = training_pos + training_neg\n",
    "dev_docs = dev_pos + dev_neg\n",
    "test_docs = test_pos + test_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"NB\"> Naive Bayes\n",
    "\n",
    "\n",
    "Feature-rich models are used to model the distribution $P_{Y|X}(y|x)$ of a target variable $y$ conditioned on some high-dimensional data $x$.\n",
    "\n",
    "One way of doing it is to summarise aspects of $x$ that are relevant to the problem by means of a feature function which returns a vector in some subset of $\\mathbb R^D$. For example, this feature function may retain sentiment words in $x$ or some other important aspects of the input. Then instead of modelling $P_{Y|X}(y|x)$ we can, for example, model $P_{Y|F_1^n}(y|f_1^n)$ where we condition on a collection of $n$ features instead.\n",
    "\n",
    "Conditioning on features of the input, rather than the input directly, does not address the problem on its own, that is, the conditioning context remains high-dimensional. But here is where we can use probability calculus and independence assumptions to make our task simpler.\n",
    "\n",
    "We can use Bayes rule to invert this conditional:\n",
    "\n",
    "\\begin{align}\n",
    "(1) \\quad P_{Y|F_1^n}(y|f_1^n) = \\frac{P_Y(y)P_{F_1^n|Y}(f_1^n|y)}{P_{F_1^n}(f_1^n)}\n",
    "\\end{align}\n",
    "\n",
    "Now note that the numerator has a conditional where the high-dimensional feature representation of the input is modelled from the target class. That is a problem we can address by making conditional independence assumptions. In particular, by making $F_i$ independent on every other $F_j$ with $i \\neq j$ given the target label $y$ we can simplify the problem a lot. We denote this by $F_i \\perp F_j \\mid y$ for $i\\neq j$. Equation (2) shows the resulting model:\n",
    "\n",
    "\\begin{align}\n",
    "(2) \\quad P_{Y|F_1^n}(y|f_1^n) \\overset{\\text{ind}}{=} \\frac{P_Y(y)\\prod_{i=1}^n P_{F|Y}(f_i|y)}{P_{F_1^n}(f_1^n)}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Note that we have to model fairly small cpds now:\n",
    "* a *prior* distribution over classes $P_Y(y)$ \n",
    "* a set of cpds $P_{F|Y}$, one per class, over the possible features (these distributions are also called likelihoods, but do not confuse it with the *likelihood function* which is a function of parameters of a statistical model for fixed data)\n",
    "* the denominator can be inferred by marginalisation, see Equation (3)\n",
    "\n",
    "\\begin{align}\n",
    "(2) \\quad P_{F_1^n}(f_1^n) = \\sum_{y \\in \\mathcal Y} P_{YF_1^n}(y, f_1^n) = \\sum_{y \\in \\mathcal Y} P_{Y}(y)P_{F_1^n|Y}(f_1^n|y) = \\sum_{y \\in \\mathcal Y} P_{Y}(y) \\prod_{i=1}^n P_{F|Y}(f_i|y)\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"ff\"> Feature function\n",
    "\n",
    "An important part of a feature-rich model such as Naive Bayes (NB) classifiers is the *feature function*. Here we will develop one. \n",
    "\n",
    "In NB classification, features are themselves random variables defined over a certain set $\\mathcal F$. We need to first determine this set. In this notebook we will focus on *unigram features*, that is, features defined at the token level.\n",
    "\n",
    "We will take every token that occurs more than a pre-specified number of times as a potential feature.\n",
    "\n",
    "Here is an example of how you can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def make_unigram_feature_set(documents, min_freq=1, mark_negation=False):\n",
    "    \"\"\"\n",
    "    This function goes through a corpus and retains all candidate unigram features\n",
    "     making a feature set. \n",
    "    \n",
    "    :param documents: all documents, each a list of words\n",
    "    :param min_freq: minimum frequency of a token for it to be part of the feature set\n",
    "    :param mark_negation: **IGNORE THIS FOR NOW**\n",
    "    :returns: unigram feature set\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    for doc in documents:\n",
    "        counter.update(doc)\n",
    "    features = []\n",
    "    for f, n in counter.most_common():\n",
    "        if n >= min_freq:\n",
    "            features.append(f)\n",
    "        else:\n",
    "            break\n",
    "    return frozenset(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex5-1\" style=\"color:red\">**Exercise 5-1**</a> **[1 points]** Modify `make_unigram_feature_set` to optionally pre-process documents by marking words in the scope of a negation with the suffix `_NEG`. For example,  `I am not sure I like the acting` becomes `I am not sure_NEG I_NEG like_NEG the_NEG acting_NEG`. You can use NLTK support for that, see for example, `nltk.sentiment.util.mark_negation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can check python documentation by using the following syntax\n",
    "from nltk.sentiment import util\n",
    "# util.mark_negation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def make_unigram_feature_set(documents, min_freq=1, mark_negation=False):\n",
    "    \"\"\"\n",
    "    This function goes through a corpus and retains all candidate unigram features\n",
    "     making a feature set. Optionally, it can also preprocess the corpus annotating\n",
    "     with _NEG words that are in the scope of a negation (using NLTK helper functions).\n",
    "    \n",
    "    :param documents: all documents, each a list of words\n",
    "    :param min_freq: minimum frequency of a token for it to be part of the feature set\n",
    "    :param mark_negation: whether to preprocess the document using NLTK's nltk.sentiment.util.mark_negation\n",
    "        see the documentation `nltk.sentiment.util.mark_negation?`\n",
    "    :returns: unigram feature set\n",
    "    \"\"\"\n",
    "    # create a counter\n",
    "    counter = Counter()\n",
    "    \n",
    "    # loop over the documents\n",
    "    for doc in documents:\n",
    "        \n",
    "        # mark negations\n",
    "        if mark_negation:\n",
    "            \n",
    "            # update the counter with the negations\n",
    "            counter.update(util.mark_negation(doc))\n",
    "            \n",
    "        # do not mark negations\n",
    "        else:\n",
    "            \n",
    "            # update the counter without negations\n",
    "            counter.update(doc)\n",
    "            \n",
    "    # features list\n",
    "    features = []\n",
    "    \n",
    "    # get the feature and occurences from the counter\n",
    "    # in order from highest to lowest n\n",
    "    for f, n in counter.most_common():\n",
    "        \n",
    "        # only include features that are frequent enough\n",
    "        if n >= min_freq:\n",
    "            \n",
    "            # add the feature to the list \n",
    "            features.append(f)\n",
    "        \n",
    "        # lower, so everything that follows will be lower\n",
    "        else:\n",
    "            \n",
    "            # break the loop\n",
    "            break\n",
    "    \n",
    "    # freeze the set of features\n",
    "    return frozenset(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just some helper code for better visualization of examples\n",
    "def inspect_set(input_set, k=5, neg=False):\n",
    "    \"\"\"\n",
    "    Helper function to inspect a few elements in a set of features\n",
    "         with _NEG words that are in the scope of a negation (using NLTK helper functions).\n",
    "    :param documents: all documents, each a list of words\n",
    "    :param input_set: a set of features\n",
    "    :param k: how many elements to select\n",
    "    :param neg: return `*_NEG` features only\n",
    "    :returns: up to k elements \n",
    "    \"\"\"\n",
    "    selected = set()\n",
    "    for w in input_set:\n",
    "        if len(selected) < k:            \n",
    "            if not neg:\n",
    "                selected.add(w)\n",
    "            elif '_NEG' in w:\n",
    "                selected.add(w)\n",
    "        else:\n",
    "            break\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the features (without marking negation):\n",
    "\n",
    "```python\n",
    ">>> unigram_features = make_unigram_feature_set(training_docs+dev_docs, min_freq=2)\n",
    ">>> print(len(unigram_features), 'features such as:\\n', inspect_set(unigram_features))\n",
    "```\n",
    "```\n",
    "9059 features such as:\n",
    " {'white', 'fearless', 'ear', 'tempting', 'meat'}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9059 features such as:\n",
      " {'barbed', 'explore', \"'\", 'partnerships', 'scenery'}\n"
     ]
    }
   ],
   "source": [
    "unigram_features = make_unigram_feature_set(training_docs+dev_docs, min_freq=2)\n",
    "print(len(unigram_features), 'features such as:\\n', inspect_set(unigram_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the features with pre-processing negation scope.\n",
    "\n",
    "```python\n",
    ">>> unigram_features_with_negation = make_unigram_feature_set(training_docs+dev_docs, min_freq=2, mark_negation=True)\n",
    ">>> print(len(unigram_features_with_negation), 'features such as:\\n', \n",
    "      inspect_set(unigram_features_with_negation), \n",
    "      '\\nand:\\n', inspect_set(unigram_features_with_negation, neg=True))\n",
    "```\n",
    "\n",
    "```\n",
    "10143 features such as:\n",
    " {'white', 'message_NEG', 'fearless', 'ear', 'tempting'} \n",
    "and:\n",
    " {'ticket_NEG', 'message_NEG', 'street_NEG', 'determined_NEG', 'stereotype_NEG'}     \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10143 features such as:\n",
      " {'word_NEG', 'explore', \"'\", 'partnerships', 'scenery'} \n",
      "and:\n",
      " {'word_NEG', 'things_NEG', 'relentlessly_NEG', 'test_NEG', 'escapism_NEG'}\n"
     ]
    }
   ],
   "source": [
    "unigram_features_with_negation = make_unigram_feature_set(training_docs+dev_docs, min_freq=2, mark_negation=True)\n",
    "print(len(unigram_features_with_negation), 'features such as:\\n', \n",
    "      inspect_set(unigram_features_with_negation), \n",
    "      '\\nand:\\n', inspect_set(unigram_features_with_negation, neg=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know which features will form the basis of our classifier, we need to implement a feature function. Here we call it a feature map (as we will be using a python dictionary).\n",
    "\n",
    "In NB classification only the features that occur in an input matter for classification, thus we use a dictionary that maps features to their values if they occur and not otherwise.\n",
    "\n",
    "This function should take a document $x$ and produce a dict where `f` (a feature) is either 1 (for binary features) or a count (for count features). For the purpose of readability we like to represent features with strings, for example:\n",
    "\n",
    "* `contains(like) = 1` means that the input contains the word `like`\n",
    "* `count(like) = 3` means that the input contains 3 occurrences of the word `like`\n",
    "* `EMPTY() = 1` means that the input contains no known feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex5-2\" style=\"color:red\">**Exercise 5-2**</a> **[2 points]** Read the documentation below and implement the feature function described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def make_feature_map(document, feature_set, \n",
    "                     binary=True, \n",
    "                     mark_negation=False):\n",
    "    \"\"\"\n",
    "    This function takes a document, possibly pre-processes it by marking words in the scope of negation, \n",
    "     and constructs a dict indicating which features in `feature_set` fire. Features may be binary, \n",
    "     flagging occurrence, or integer, indicating the number of occurrences.\n",
    "     If no feature can be extracted, a special feature is fired, namely 'EMPTY()'.\n",
    "     \n",
    "    :param document: a list of words\n",
    "    :param feature_set: set of features we are looking for\n",
    "    :param binary: whether we are indicating presence or counting features in feature_set\n",
    "    :param mark_negation: whether we should apply NLTK's mark_negation to document before applying the feature function\n",
    "    :returns: dict with entries 'contains(f)=1/0' for binary features or 'count(f)=n' for count features\n",
    "    \"\"\"\n",
    "    \n",
    "    # create the map\n",
    "    feature_map = defaultdict(float)\n",
    "    \n",
    "    # checks if the negations should be marked\n",
    "    if mark_negation:\n",
    "        \n",
    "        # mark the negations in the document\n",
    "        document = util.mark_negation(document)\n",
    "    \n",
    "    # loop over the words in the document\n",
    "    for word in document:\n",
    "        \n",
    "        # check if the word is in the feature set\n",
    "        if word in feature_set:\n",
    "            \n",
    "            # make a binary feature map\n",
    "            if binary:\n",
    "                \n",
    "                # add the word to the map\n",
    "                feature_map['contains(' + word + ')'] = 1.0\n",
    "                \n",
    "            # make a counting feature map\n",
    "            else:\n",
    "    \n",
    "                # add a count of the word to the map\n",
    "                feature_map['count(' + word + ')'] += 1.0\n",
    "    \n",
    "    # check if the map is empty\n",
    "    if not feature_map:\n",
    "        \n",
    "        # mark it as empty\n",
    "        feature_map['EMPTY()'] = 1.0\n",
    "       \n",
    "    # return the map\n",
    "    return feature_map\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some outputs for you to check your implementation:\n",
    "\n",
    "```python\n",
    ">>> make_feature_map(pos_sents[7], unigram_features_with_negation, \n",
    "                 binary=True, mark_negation=True)\n",
    "```\n",
    "```\n",
    "defaultdict(float,\n",
    "            {'contains(.)': 1.0,\n",
    "             'contains(ever_NEG)': 1.0,\n",
    "             'contains(good_NEG)': 1.0,\n",
    "             'contains(has_NEG)': 1.0,\n",
    "             'contains(hell_NEG)': 1.0,\n",
    "             'contains(is_NEG)': 1.0,\n",
    "             'contains(literally_NEG)': 1.0,\n",
    "             'contains(made_NEG)': 1.0,\n",
    "             'contains(more_NEG)': 1.0,\n",
    "             'contains(no)': 1.0,\n",
    "             'contains(perhaps)': 1.0,\n",
    "             'contains(picture_NEG)': 1.0,\n",
    "             'contains(road_NEG)': 1.0,\n",
    "             'contains(that_NEG)': 1.0,\n",
    "             'contains(the_NEG)': 1.0,\n",
    "             'contains(to_NEG)': 1.0,\n",
    "             'contains(with_NEG)': 1.0})\n",
    "```\n",
    "```python\n",
    ">>> make_feature_map(['AKSJDHAU'], unigram_features_with_negation, \n",
    "                 binary=True, mark_negation=True)\n",
    "```\n",
    "```\n",
    "defaultdict(float, {'EMPTY()': 1.0})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {'contains(.)': 1.0,\n",
       "             'contains(ever_NEG)': 1.0,\n",
       "             'contains(good_NEG)': 1.0,\n",
       "             'contains(has_NEG)': 1.0,\n",
       "             'contains(hell_NEG)': 1.0,\n",
       "             'contains(is_NEG)': 1.0,\n",
       "             'contains(literally_NEG)': 1.0,\n",
       "             'contains(made_NEG)': 1.0,\n",
       "             'contains(more_NEG)': 1.0,\n",
       "             'contains(no)': 1.0,\n",
       "             'contains(perhaps)': 1.0,\n",
       "             'contains(picture_NEG)': 1.0,\n",
       "             'contains(road_NEG)': 1.0,\n",
       "             'contains(that_NEG)': 1.0,\n",
       "             'contains(the_NEG)': 1.0,\n",
       "             'contains(to_NEG)': 1.0,\n",
       "             'contains(with_NEG)': 1.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_feature_map(pos_sents[7], unigram_features_with_negation, binary=True, mark_negation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float, {'EMPTY()': 1.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_feature_map(['AKSJDHAU'], unigram_features_with_negation, binary=True, mark_negation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {'count(\")': 4.0,\n",
       "             'count(,)': 3.0,\n",
       "             'count(--)': 1.0,\n",
       "             'count(.)': 13.0,\n",
       "             'count(21st)': 1.0,\n",
       "             'count(;)': 1.0,\n",
       "             'count(a)': 5.0,\n",
       "             'count(absolute)': 1.0,\n",
       "             'count(adequately)': 1.0,\n",
       "             'count(all)': 1.0,\n",
       "             'count(an)': 1.0,\n",
       "             'count(and)': 3.0,\n",
       "             'count(arnold)': 1.0,\n",
       "             'count(as)': 1.0,\n",
       "             'count(asian)': 1.0,\n",
       "             'count(at)': 1.0,\n",
       "             'count(be)': 1.0,\n",
       "             'count(biopic)': 1.0,\n",
       "             'count(but)': 2.0,\n",
       "             'count(cannot)': 1.0,\n",
       "             'count(care)': 1.0,\n",
       "             'count(cat)': 1.0,\n",
       "             'count(cinema)': 1.0,\n",
       "             'count(clever)': 1.0,\n",
       "             'count(co-writer/director)': 1.0,\n",
       "             'count(column)': 1.0,\n",
       "             'count(combination)': 1.0,\n",
       "             'count(comics)': 1.0,\n",
       "             'count(conan)': 1.0,\n",
       "             'count(continuation)': 1.0,\n",
       "             'count(damme)': 1.0,\n",
       "             'count(describe)': 1.0,\n",
       "             'count(destined)': 1.0,\n",
       "             'count(different)': 1.0,\n",
       "             \"count(doesn't)\": 1.0,\n",
       "             'count(edges)': 1.0,\n",
       "             'count(education)': 1.0,\n",
       "             'count(effective)': 1.0,\n",
       "             'count(elaborate)': 1.0,\n",
       "             'count(emerges)': 1.0,\n",
       "             'count(entertainment)': 1.0,\n",
       "             'count(even)': 2.0,\n",
       "             'count(ever_NEG)': 1.0,\n",
       "             'count(expanded)': 1.0,\n",
       "             'count(feel_NEG)': 1.0,\n",
       "             'count(film)': 1.0,\n",
       "             'count(fun)': 1.0,\n",
       "             'count(game)': 1.0,\n",
       "             'count(go)': 1.0,\n",
       "             'count(going)': 1.0,\n",
       "             'count(good)': 1.0,\n",
       "             'count(good_NEG)': 1.0,\n",
       "             'count(gorgeously)': 1.0,\n",
       "             'count(great)': 1.0,\n",
       "             'count(greater)': 1.0,\n",
       "             'count(has_NEG)': 1.0,\n",
       "             'count(hate)': 1.0,\n",
       "             'count(have)': 2.0,\n",
       "             \"count(he's)\": 1.0,\n",
       "             'count(he)': 1.0,\n",
       "             'count(hell_NEG)': 1.0,\n",
       "             'count(honest)': 1.0,\n",
       "             'count(huge)': 1.0,\n",
       "             'count(if)': 1.0,\n",
       "             'count(in)': 1.0,\n",
       "             'count(insight)': 1.0,\n",
       "             'count(into)': 1.0,\n",
       "             'count(is)': 3.0,\n",
       "             'count(is_NEG)': 1.0,\n",
       "             'count(issue)': 1.0,\n",
       "             \"count(it's)\": 1.0,\n",
       "             'count(it)': 3.0,\n",
       "             'count(j)': 1.0,\n",
       "             \"count(jackson's)\": 1.0,\n",
       "             'count(keenly)': 1.0,\n",
       "             'count(like)': 1.0,\n",
       "             'count(like_NEG)': 1.0,\n",
       "             'count(literally_NEG)': 1.0,\n",
       "             'count(lord)': 1.0,\n",
       "             'count(made_NEG)': 1.0,\n",
       "             'count(make)': 1.0,\n",
       "             'count(mindset)': 1.0,\n",
       "             'count(more_NEG)': 1.0,\n",
       "             'count(movie)': 1.0,\n",
       "             'count(movies)': 1.0,\n",
       "             'count(my)': 1.0,\n",
       "             'count(neurotic)': 1.0,\n",
       "             'count(new)': 1.0,\n",
       "             'count(no)': 1.0,\n",
       "             'count(observed)': 1.0,\n",
       "             'count(of)': 9.0,\n",
       "             'count(off)': 1.0,\n",
       "             'count(offers)': 2.0,\n",
       "             'count(one_NEG)': 1.0,\n",
       "             'count(or)': 1.0,\n",
       "             'count(perhaps)': 1.0,\n",
       "             'count(peter)': 1.0,\n",
       "             'count(picture_NEG)': 1.0,\n",
       "             'count(place)': 1.0,\n",
       "             'count(provides)': 1.0,\n",
       "             'count(pulls)': 1.0,\n",
       "             'count(r)': 2.0,\n",
       "             'count(rare)': 2.0,\n",
       "             'count(reached)': 1.0,\n",
       "             'count(refreshingly)': 1.0,\n",
       "             'count(rings)': 1.0,\n",
       "             'count(road_NEG)': 1.0,\n",
       "             'count(rock)': 1.0,\n",
       "             'count(schwarzenegger)': 1.0,\n",
       "             'count(screenplay)': 1.0,\n",
       "             'count(slice)': 1.0,\n",
       "             'count(snappy)': 1.0,\n",
       "             'count(so)': 3.0,\n",
       "             'count(some)': 1.0,\n",
       "             'count(somehow)': 1.0,\n",
       "             'count(something)': 1.0,\n",
       "             'count(sometimes)': 1.0,\n",
       "             'count(splash)': 1.0,\n",
       "             'count(start)': 1.0,\n",
       "             'count(steers)': 1.0,\n",
       "             'count(steven)': 1.0,\n",
       "             'count(take)': 1.0,\n",
       "             'count(than)': 1.0,\n",
       "             \"count(that's)\": 1.0,\n",
       "             'count(that)': 5.0,\n",
       "             'count(that_NEG)': 1.0,\n",
       "             'count(the)': 11.0,\n",
       "             'count(the_NEG)': 1.0,\n",
       "             'count(those)': 1.0,\n",
       "             'count(to)': 7.0,\n",
       "             'count(to_NEG)': 1.0,\n",
       "             \"count(tolkien's)\": 1.0,\n",
       "             'count(top)': 1.0,\n",
       "             'count(trilogy)': 1.0,\n",
       "             'count(turns)': 1.0,\n",
       "             'count(van)': 1.0,\n",
       "             'count(vision)': 1.0,\n",
       "             'count(want)': 1.0,\n",
       "             'count(wasabi)': 1.0,\n",
       "             'count(who)': 1.0,\n",
       "             'count(with_NEG)': 1.0,\n",
       "             'count(words)': 1.0,\n",
       "             'count(you)': 2.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_feature_map([word for sentence in pos_sents[:10] for word in sentence], \n",
    "                 unigram_features_with_negation, binary=False, mark_negation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"MLE\"> MLE\n",
    "\n",
    "Now you will estimate the cpds involved in NB classification. We use a balanced dataset over two classes (positive and negative), so there's no need to compute $P_Y(y)$, it would simply be $0.5$ per class.\n",
    "\n",
    "You should simply implement cpds for $P_{F|Y}$, that is, exactly $2$ cpds via MLE and you should use Laplace-$\\alpha$ smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex5-3\" style=\"color:red\">**Exercise 5-3**</a> **[3 points]** Check the documentation below and complete the code for the NB classifier. You will need to implement\n",
    "\n",
    "* estimation of cpds $P_{F|Y}$ with Laplace smoothing  (1 point) \n",
    "* the `classify` method (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_cpd(raw_counts, alpha, v):\n",
    "    \"\"\"\n",
    "    This converts a dictionary of raw counts into a cpd.\n",
    "\n",
    "    :param raw_counts: dict where a key is a feature and a value is its counts (without pseudo counts)\n",
    "        this should already include the 'EMPTY()' feature\n",
    "    :param alpha: how many pseudo counts should we add per observation\n",
    "    :param v: the size of the feature set (already including the 'EMPTY()' feature)\n",
    "    :returns: a cpd as a dict where a key is a feature and a value is its smoothed probability\n",
    "    \"\"\"\n",
    "    \n",
    "    # create the cpd dict\n",
    "    cpd = defaultdict(float)      \n",
    "    \n",
    "    # get the total number of words\n",
    "    total = sum(raw_counts.values())\n",
    "    \n",
    "    # loop over the counted words\n",
    "    for word, count in raw_counts.items():\n",
    "\n",
    "        # get the smoothed probability ( count + alpha / N + v * alpha )\n",
    "        cpd[word] = (count + alpha) / (total + ((v-1)*alpha))\n",
    "   \n",
    "    # return the cpd\n",
    "    return cpd\n",
    "\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    \n",
    "    def __init__(self, training_pos, training_neg, binary, mark_negation, alpha=0.1, min_freq=2):\n",
    "        \"\"\"\n",
    "        :param training_pos: positive documents\n",
    "            a document is a list of tokens\n",
    "        :param training_neg: negative documents\n",
    "            a document is a list of tokens\n",
    "        :param binary: whether we are using binary or count features\n",
    "        :param mark_negation: whether we are pre-processing words in negation scope\n",
    "        :param alpha: Laplace smooth pseudo count\n",
    "        :param min_freq: minimum frequency of a token for it to be considered a feature\n",
    "        \"\"\"\n",
    "                \n",
    "        # Make feature set\n",
    "        print('Extracting features:')\n",
    "        feature_set = make_unigram_feature_set(\n",
    "            training_pos + training_neg,  # we use a concatenation of positive and negative training instances\n",
    "            min_freq=min_freq, \n",
    "            mark_negation=mark_negation)\n",
    "        \n",
    "        print(' %d features' % len(feature_set))\n",
    "                \n",
    "        # Estimate model: 1/2) count        \n",
    "        print('MLE: counting')        \n",
    "        counts = [defaultdict(float), defaultdict(float)]\n",
    "        for docs, y in [(training_pos, 1), (training_neg, 0)]:\n",
    "            for doc in docs:  # for each document\n",
    "                # we extract features\n",
    "                fmap = make_feature_map(doc, \n",
    "                                        feature_set, \n",
    "                                        binary=binary, \n",
    "                                        mark_negation=mark_negation)\n",
    "                # and gather counts for the pair (y, f)\n",
    "                for f, n in fmap.items():\n",
    "                    counts[y][f] += n  \n",
    "                                \n",
    "        # 2/2) Laplace-1 MLE\n",
    "        #  we put EMPTY() is in the support\n",
    "        print('MLE: smoothing')\n",
    "        counts[0]['EMPTY()'] += 0\n",
    "        counts[1]['EMPTY()'] += 0\n",
    "        # and compute cpds using Laplace smoothing\n",
    "        self._cpds = [\n",
    "            make_cpd(counts[0], alpha, len(feature_set) + 1),  # we add 1 because we want EMPTY() to add towards total\n",
    "            make_cpd(counts[1], alpha, len(feature_set) + 1)]\n",
    "        print('MLE: done')\n",
    "            \n",
    "        # Store data\n",
    "        self._binary = binary\n",
    "        self._mark_negation = mark_negation\n",
    "        self._alpha = alpha\n",
    "        self._feature_set = feature_set\n",
    "            \n",
    "    def get_log_parameter(self, f, y):\n",
    "        \"\"\"Returns log P(f|y)\"\"\"\n",
    "        return np.log(self._cpds[y].get(f, self._cpds[y]['EMPTY()']))\n",
    "        \n",
    "    def classify(self, doc):\n",
    "        \"\"\"\n",
    "        This function classifies a document by extracting features <f_1...f_n> for it \n",
    "         and then computing \n",
    "            log P(<f_1...f_n>|Y=0) and log P(<f_1...f_n>|Y=1)\n",
    "         and finally picking the best (that is, either Y=0 or Y=1).\n",
    "        \n",
    "        :param doc: a list of tokens\n",
    "        :returns: 0 or 1 (the argmax of log P(<f_1...f_n>|y))\n",
    "        \"\"\"\n",
    "        \n",
    "        # initial probabilities\n",
    "        probabilities = [0, 0]\n",
    "        \n",
    "        # create a feature map of the document\n",
    "        fmap = make_feature_map(doc, feature_set=self._feature_set, \n",
    "                                binary=self._binary, mark_negation=self._mark_negation)\n",
    "            \n",
    "        # loop over the words in the feature map\n",
    "        for key in fmap: \n",
    "            \n",
    "            # get the log prob of the negative sentiment\n",
    "            probabilities[0] += self.get_log_parameter(key, 0)\n",
    "\n",
    "            #  get the log prob of the positive sentiment\n",
    "            probabilities[1] += self.get_log_parameter(key, 1)\n",
    "            \n",
    "        # return the index of the highest probability (or 0 if they are equal)\n",
    "        return np.argmax(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how you should use the classifier:\n",
    "\n",
    "```python\n",
    ">>> classifier1 = NaiveBayesClassifier(\n",
    "    training_pos, training_neg, \n",
    "    binary=True, mark_negation=False,\n",
    "    alpha=1., min_freq=2)\n",
    "```\n",
    "```\n",
    "Extracting features:\n",
    " 8577 features\n",
    "MLE: counting\n",
    "MLE: smoothing\n",
    "MLE: done\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features:\n",
      " 8577 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n"
     ]
    }
   ],
   "source": [
    "classifier1 = NaiveBayesClassifier(\n",
    "    training_pos, training_neg, \n",
    "    binary=True, mark_negation=False,\n",
    "    alpha=1., min_freq=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"eval\"> Evaluation\n",
    "\n",
    "We evaluate binary classifiers on precision, recall, F1 and accuracy. See [Figure 4.4](https://web.stanford.edu/~jurafsky/slp3/4.pdf) and complete the code below:\n",
    "\n",
    "<a name=\"ex5-4\" style=\"color:red\">**Exercise 5-4**</a> **[2 points]** Classify all documents in a dev set and compute the quantities in [Figure 4.4](https://web.stanford.edu/~jurafsky/slp3/4.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(classifier, pos_docs, neg_docs):\n",
    "    \"\"\"\n",
    "    :param classifier: an NaiveBayesClassifier object\n",
    "    :param pos_docs: positive documents\n",
    "    :param neg_docs: negative documents\n",
    "    :returns: a dictionary containing the number of\n",
    "        * true positives\n",
    "        * true negatives\n",
    "        * false positives\n",
    "        * false negatives\n",
    "     as well as \n",
    "        * accuracy\n",
    "        * precision\n",
    "        * recall \n",
    "        * and [F1](https://en.wikipedia.org/wiki/F1_score)\n",
    "    \"\"\"\n",
    "    \n",
    "    # initial values for TP, TN, FP, FN\n",
    "    true_positives = 0\n",
    "    true_negatives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    \n",
    "    # loop over the positive documents\n",
    "    for doc in pos_docs:\n",
    "        \n",
    "        # classify the document\n",
    "        classification = classifier.classify(doc)\n",
    "        \n",
    "        # if the classification is positive mark it as TP\n",
    "        if classification == 1:\n",
    "            true_positives += 1\n",
    "        \n",
    "        # otherwise it is a FN\n",
    "        else:\n",
    "            false_negatives += 1\n",
    "            \n",
    "    # loop over the negative docs\n",
    "    for doc in neg_docs:\n",
    "        \n",
    "        # classify the document\n",
    "        classification = classifier.classify(doc)\n",
    "        \n",
    "        # if the classification is negative, mark it as TN\n",
    "        if classification == 0:\n",
    "            true_negatives += 1\n",
    "            \n",
    "        # otherwise it is a FP\n",
    "        else:\n",
    "            false_positives += 1\n",
    "            \n",
    "    # accuracy is ( TP + TN ) / ( TP + TN + FP + FN )\n",
    "    accuracy = float(true_positives + true_negatives) / (true_positives + true_negatives + \n",
    "                                                    false_positives + false_negatives)\n",
    "    \n",
    "    # precision is TP / ( TP + FP )\n",
    "    precision = float(true_positives) / (true_positives + false_positives)\n",
    "    \n",
    "    # recall is TP / ( TP + FN )\n",
    "    recall = float(true_positives) / (true_positives + false_negatives)\n",
    "    \n",
    "    # F1-score is 2 * P * R  / ( P + R )\n",
    "    f1_score = 2 * ((precision * recall) / (precision + recall))\n",
    "    \n",
    "    # create the result dictionary\n",
    "    result = dict()\n",
    "    \n",
    "    # fill in the dict\n",
    "    result['TP'] = true_positives\n",
    "    result['TN'] = true_negatives\n",
    "    result['FP'] = false_positives\n",
    "    result['FN'] = false_negatives\n",
    "    result['A'] = accuracy\n",
    "    result['P'] = precision\n",
    "    result['R'] = recall\n",
    "    result['F1'] = f1_score\n",
    "    \n",
    "    #  return the dict\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, our implementation yields:\n",
    "\n",
    "```python\n",
    ">>> classifier1 = NaiveBayesClassifier(\n",
    "    training_pos, training_neg, \n",
    "    binary=True, mark_negation=False,\n",
    "    alpha=1., min_freq=2)\n",
    "```\n",
    "```\n",
    "Extracting features:\n",
    " 8577 features\n",
    "MLE: counting\n",
    "MLE: smoothing\n",
    "MLE: done\n",
    "```\n",
    "```python\n",
    ">>> dev_metrics1 = evaluate_model(classifier1, dev_pos, dev_neg)\n",
    ">>> print('Development')\n",
    ">>> print('TP %d TN %d FP %d FN %d' % (dev_metrics1['TP'], dev_metrics1['TN'], dev_metrics1['FP'], dev_metrics1['FN']))\n",
    ">>> print('P %.4f R %.4f A %.4f F1 %.4f' % (dev_metrics1['P'], dev_metrics1['R'], dev_metrics1['A'], dev_metrics1['F1']))\n",
    "```\n",
    "```\n",
    "Development\n",
    "TP 239 TN 268 FP 63 FN 92\n",
    "P 0.7914 R 0.7221 A 0.7659 F1 0.7551\n",
    "```\n",
    "```python\n",
    ">>> classifier2 = NaiveBayesClassifier(\n",
    "    training_pos, training_neg, \n",
    "    binary=True, mark_negation=True,\n",
    "    alpha=1., min_freq=2)\n",
    "```\n",
    "```\n",
    "Extracting features:\n",
    " 9581 features\n",
    "MLE: counting\n",
    "MLE: smoothing\n",
    "MLE: done  \n",
    "```\n",
    "```python\n",
    ">>> dev_metrics2 = evaluate_model(classifier2, dev_pos, dev_neg)\n",
    ">>> print('Development')\n",
    ">>> print('TP %d TN %d FP %d FN %d' % (dev_metrics2['TP'], dev_metrics2['TN'], dev_metrics2['FP'], dev_metrics2['FN']))\n",
    ">>> print('P %.4f R %.4f A %.4f F1 %.4f' % (dev_metrics2['P'], dev_metrics2['R'], dev_metrics2['A'], dev_metrics2['F1']))\n",
    "```\n",
    "```\n",
    "Development\n",
    "TP 248 TN 273 FP 58 FN 83\n",
    "P 0.8105 R 0.7492 A 0.7870 F1 0.7786\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features:\n",
      " 8577 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n"
     ]
    }
   ],
   "source": [
    "classifier1 = NaiveBayesClassifier(\n",
    "    training_pos, training_neg, \n",
    "    binary=True, mark_negation=False,\n",
    "    alpha=1., min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development\n",
      "TP 239 TN 268 FP 63 FN 92\n",
      "P 0.7914 R 0.7221 A 0.7659 F1 0.7551\n"
     ]
    }
   ],
   "source": [
    "dev_metrics1 = evaluate_model(classifier1, dev_pos, dev_neg)\n",
    "print('Development')\n",
    "print('TP %d TN %d FP %d FN %d' % (dev_metrics1['TP'], dev_metrics1['TN'], dev_metrics1['FP'], dev_metrics1['FN']))\n",
    "print('P %.4f R %.4f A %.4f F1 %.4f' % (dev_metrics1['P'], dev_metrics1['R'], dev_metrics1['A'], dev_metrics1['F1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features:\n",
      " 9581 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n"
     ]
    }
   ],
   "source": [
    "classifier2 = NaiveBayesClassifier(\n",
    "    training_pos, training_neg, \n",
    "    binary=True, mark_negation=True,\n",
    "    alpha=1., min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development\n",
      "TP 248 TN 273 FP 58 FN 83\n",
      "P 0.8105 R 0.7492 A 0.7870 F1 0.7786\n"
     ]
    }
   ],
   "source": [
    "dev_metrics2 = evaluate_model(classifier2, dev_pos, dev_neg)\n",
    "print('Development')\n",
    "print('TP %d TN %d FP %d FN %d' % (dev_metrics2['TP'], dev_metrics2['TN'], dev_metrics2['FP'], dev_metrics2['FN']))\n",
    "print('P %.4f R %.4f A %.4f F1 %.4f' % (dev_metrics2['P'], dev_metrics2['R'], dev_metrics2['A'], dev_metrics2['F1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex5-5\" style=\"color:red\">**Exercise 5-5**</a> **[4 points]** Use the dev set to choose the best configuration of \n",
    "\n",
    "* alpha (try values like 0.1, 0.5, 1.)\n",
    "* and binary vs count\n",
    "\n",
    "for a model that marks negation and one that does not. \n",
    "\n",
    "Then report performance on test set for your best model in each case. \n",
    "\n",
    "Points:\n",
    "* 3 points for the search on dev set\n",
    "* 1 point for the table of test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameters(alpha, binary, mark_negation, min_freq):\n",
    "    \"\"\"\n",
    "    Get the best parameters for the NB classifier\n",
    "    :param alpha: the pseudo count for smoothing \n",
    "    :param binary: binary feature map or not\n",
    "    :param mark_negation: mark the negations or not\n",
    "    :param min_freq: minimum frequency for words to include them\n",
    "    :returns: a list of dictionaries with the results and\n",
    "    \n",
    "    NOTE: the parameters are lists with options that are evaluated\n",
    "    \"\"\"\n",
    "    # result list\n",
    "    result = []\n",
    "    \n",
    "    # loop over the alpha's\n",
    "    for a in alpha:\n",
    "        \n",
    "        # try the binary options\n",
    "        for b in binary:\n",
    "            \n",
    "            # try the mark negation options\n",
    "            for mn in mark_negation:\n",
    "                \n",
    "                # loop over the min frequencies\n",
    "                for mf in min_freq:\n",
    "                    \n",
    "                    # create the classifier\n",
    "                    classifier = NaiveBayesClassifier(\n",
    "                    training_pos, training_neg, \n",
    "                    binary=b, mark_negation=mn,\n",
    "                    alpha=a, min_freq=mf)\n",
    "                    \n",
    "                    # evaluate the model\n",
    "                    dev_metrics = evaluate_model(classifier, dev_pos, dev_neg)\n",
    "                    \n",
    "                    # add the current parameters to the results\n",
    "                    dev_metrics['Alpha'] = a\n",
    "                    dev_metrics['Binary'] = b\n",
    "                    dev_metrics['Mark Negation'] = mn\n",
    "                    dev_metrics['Min Frequency'] = mf\n",
    "                    \n",
    "                    # add the current result to the list\n",
    "                    result.append(dev_metrics)\n",
    "    \n",
    "    # return the list\n",
    "    return np.asarray(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features:\n",
      " 21805 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:85: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9581 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 6283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4701 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3739 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 18283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 8577 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 5829 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4448 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3525 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 21805 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 9581 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 6283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4701 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3739 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 18283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 8577 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 5829 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4448 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3525 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 21805 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 9581 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 6283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4701 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3739 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 18283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 8577 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 5829 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4448 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3525 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 21805 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 9581 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 6283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4701 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3739 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 18283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 8577 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 5829 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4448 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3525 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 21805 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 9581 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 6283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4701 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3739 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 18283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 8577 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 5829 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4448 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3525 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 21805 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 9581 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 6283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4701 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3739 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 18283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 8577 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 5829 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4448 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3525 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 21805 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 9581 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 6283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4701 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3739 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 18283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 8577 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 5829 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4448 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3525 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 21805 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 9581 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 6283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4701 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3739 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 18283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 8577 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 5829 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4448 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3525 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 21805 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 9581 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 6283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4701 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3739 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 18283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 8577 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 5829 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4448 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3525 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 21805 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 9581 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 6283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4701 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3739 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 18283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 8577 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 5829 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4448 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3525 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 21805 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 9581 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 6283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4701 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3739 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 18283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 8577 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 5829 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4448 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3525 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 21805 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 9581 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 6283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4701 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3739 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 18283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 8577 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 5829 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4448 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3525 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 21805 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 9581 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 6283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4701 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3739 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 18283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 8577 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 5829 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4448 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3525 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 21805 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 9581 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 6283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4701 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3739 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 18283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 8577 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 5829 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4448 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3525 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 21805 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 9581 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 6283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4701 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3739 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 18283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 8577 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 5829 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4448 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3525 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 21805 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 9581 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 6283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4701 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3739 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 18283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 8577 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 5829 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4448 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3525 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 21805 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 9581 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 6283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4701 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3739 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 18283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 8577 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 5829 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4448 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3525 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 21805 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 9581 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 6283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4701 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3739 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 18283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 8577 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 5829 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4448 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3525 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 21805 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 9581 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 6283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4701 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3739 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 18283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 8577 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 5829 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4448 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3525 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 21805 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 9581 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 6283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4701 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3739 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 18283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 8577 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 5829 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4448 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3525 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 21805 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 9581 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 6283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4701 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3739 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 18283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 8577 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 5829 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4448 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3525 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 21805 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 9581 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 6283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4701 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3739 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 18283 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 8577 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 5829 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 4448 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n",
      "Extracting features:\n",
      " 3525 features\n",
      "MLE: counting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE: smoothing\n",
      "MLE: done\n"
     ]
    }
   ],
   "source": [
    "# alpha values of 0, 0.1, 0.2, ..., 1.0\n",
    "alpha = np.linspace(0, 1, 11)\n",
    "\n",
    "# both options for the booleans\n",
    "binary = [True, False]\n",
    "mark_negation = [True, False]\n",
    "\n",
    "# min frequency from 1 to 5\n",
    "min_freq = range(1, 6)\n",
    "\n",
    "# get all the results from the given parameter options\n",
    "results = hyperparameters(alpha, binary, mark_negation, min_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best parameters for a certain score\n",
    "def get_best_parameters(data, score):\n",
    "    \n",
    "    # get the index of the best parameters\n",
    "    index = np.argmax([x[score] for x in data])\n",
    "    \n",
    "    # return the results with the best parameters\n",
    "    return data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TP': 258, 'TN': 262, 'FP': 69, 'FN': 73, 'A': 0.7854984894259819, 'P': 0.7889908256880734, 'R': 0.7794561933534743, 'F1': 0.7841945288753798, 'Alpha': 0.30000000000000004, 'Binary': False, 'Mark Negation': True, 'Min Frequency': 3}\n"
     ]
    }
   ],
   "source": [
    "# the best f1 score\n",
    "best_F1 = get_best_parameters(results, 'F1')\n",
    "print(best_F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TP': 250, 'TN': 273, 'FP': 58, 'FN': 81, 'A': 0.7900302114803626, 'P': 0.8116883116883117, 'R': 0.7552870090634441, 'F1': 0.7824726134585289, 'Alpha': 0.5, 'Binary': True, 'Mark Negation': True, 'Min Frequency': 2}\n"
     ]
    }
   ],
   "source": [
    "# the best accuracy\n",
    "best_A = get_best_parameters(results, 'A')\n",
    "print(best_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TP': 247, 'TN': 274, 'FP': 57, 'FN': 84, 'A': 0.7870090634441088, 'P': 0.8125, 'R': 0.7462235649546828, 'F1': 0.7779527559055119, 'Alpha': 0.5, 'Binary': False, 'Mark Negation': True, 'Min Frequency': 2}\n"
     ]
    }
   ],
   "source": [
    "# the best precision\n",
    "best_P = get_best_parameters(results, 'P')\n",
    "print(best_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TP': 259, 'TN': 242, 'FP': 89, 'FN': 72, 'A': 0.756797583081571, 'P': 0.7442528735632183, 'R': 0.7824773413897281, 'F1': 0.7628865979381443, 'Alpha': 0.0, 'Binary': True, 'Mark Negation': True, 'Min Frequency': 3}\n"
     ]
    }
   ],
   "source": [
    "# the best recall\n",
    "best_R = get_best_parameters(results, 'R')\n",
    "print(best_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>A</th>\n",
       "      <th>P</th>\n",
       "      <th>R</th>\n",
       "      <th>F1</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Binary</th>\n",
       "      <th>Mark Negation</th>\n",
       "      <th>Min Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>183</td>\n",
       "      <td>267</td>\n",
       "      <td>64</td>\n",
       "      <td>148</td>\n",
       "      <td>0.679758</td>\n",
       "      <td>0.740891</td>\n",
       "      <td>0.552870</td>\n",
       "      <td>0.633218</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>224</td>\n",
       "      <td>259</td>\n",
       "      <td>72</td>\n",
       "      <td>107</td>\n",
       "      <td>0.729607</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.676737</td>\n",
       "      <td>0.714514</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>259</td>\n",
       "      <td>242</td>\n",
       "      <td>89</td>\n",
       "      <td>72</td>\n",
       "      <td>0.756798</td>\n",
       "      <td>0.744253</td>\n",
       "      <td>0.782477</td>\n",
       "      <td>0.762887</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>249</td>\n",
       "      <td>253</td>\n",
       "      <td>78</td>\n",
       "      <td>82</td>\n",
       "      <td>0.758308</td>\n",
       "      <td>0.761468</td>\n",
       "      <td>0.752266</td>\n",
       "      <td>0.756839</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>237</td>\n",
       "      <td>251</td>\n",
       "      <td>80</td>\n",
       "      <td>94</td>\n",
       "      <td>0.737160</td>\n",
       "      <td>0.747634</td>\n",
       "      <td>0.716012</td>\n",
       "      <td>0.731481</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>177</td>\n",
       "      <td>258</td>\n",
       "      <td>73</td>\n",
       "      <td>154</td>\n",
       "      <td>0.657100</td>\n",
       "      <td>0.708000</td>\n",
       "      <td>0.534743</td>\n",
       "      <td>0.609294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>220</td>\n",
       "      <td>257</td>\n",
       "      <td>74</td>\n",
       "      <td>111</td>\n",
       "      <td>0.720544</td>\n",
       "      <td>0.748299</td>\n",
       "      <td>0.664653</td>\n",
       "      <td>0.704000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>251</td>\n",
       "      <td>243</td>\n",
       "      <td>88</td>\n",
       "      <td>80</td>\n",
       "      <td>0.746224</td>\n",
       "      <td>0.740413</td>\n",
       "      <td>0.758308</td>\n",
       "      <td>0.749254</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>243</td>\n",
       "      <td>253</td>\n",
       "      <td>78</td>\n",
       "      <td>88</td>\n",
       "      <td>0.749245</td>\n",
       "      <td>0.757009</td>\n",
       "      <td>0.734139</td>\n",
       "      <td>0.745399</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>234</td>\n",
       "      <td>257</td>\n",
       "      <td>74</td>\n",
       "      <td>97</td>\n",
       "      <td>0.741692</td>\n",
       "      <td>0.759740</td>\n",
       "      <td>0.706949</td>\n",
       "      <td>0.732394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>182</td>\n",
       "      <td>268</td>\n",
       "      <td>63</td>\n",
       "      <td>149</td>\n",
       "      <td>0.679758</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.549849</td>\n",
       "      <td>0.631944</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>224</td>\n",
       "      <td>259</td>\n",
       "      <td>72</td>\n",
       "      <td>107</td>\n",
       "      <td>0.729607</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.676737</td>\n",
       "      <td>0.714514</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>259</td>\n",
       "      <td>243</td>\n",
       "      <td>88</td>\n",
       "      <td>72</td>\n",
       "      <td>0.758308</td>\n",
       "      <td>0.746398</td>\n",
       "      <td>0.782477</td>\n",
       "      <td>0.764012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>248</td>\n",
       "      <td>255</td>\n",
       "      <td>76</td>\n",
       "      <td>83</td>\n",
       "      <td>0.759819</td>\n",
       "      <td>0.765432</td>\n",
       "      <td>0.749245</td>\n",
       "      <td>0.757252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>236</td>\n",
       "      <td>253</td>\n",
       "      <td>78</td>\n",
       "      <td>95</td>\n",
       "      <td>0.738671</td>\n",
       "      <td>0.751592</td>\n",
       "      <td>0.712991</td>\n",
       "      <td>0.731783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>177</td>\n",
       "      <td>259</td>\n",
       "      <td>72</td>\n",
       "      <td>154</td>\n",
       "      <td>0.658610</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.534743</td>\n",
       "      <td>0.610345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>221</td>\n",
       "      <td>259</td>\n",
       "      <td>72</td>\n",
       "      <td>110</td>\n",
       "      <td>0.725076</td>\n",
       "      <td>0.754266</td>\n",
       "      <td>0.667674</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>250</td>\n",
       "      <td>246</td>\n",
       "      <td>85</td>\n",
       "      <td>81</td>\n",
       "      <td>0.749245</td>\n",
       "      <td>0.746269</td>\n",
       "      <td>0.755287</td>\n",
       "      <td>0.750751</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>242</td>\n",
       "      <td>257</td>\n",
       "      <td>74</td>\n",
       "      <td>89</td>\n",
       "      <td>0.753776</td>\n",
       "      <td>0.765823</td>\n",
       "      <td>0.731118</td>\n",
       "      <td>0.748068</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>232</td>\n",
       "      <td>260</td>\n",
       "      <td>71</td>\n",
       "      <td>99</td>\n",
       "      <td>0.743202</td>\n",
       "      <td>0.765677</td>\n",
       "      <td>0.700906</td>\n",
       "      <td>0.731861</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>245</td>\n",
       "      <td>260</td>\n",
       "      <td>71</td>\n",
       "      <td>86</td>\n",
       "      <td>0.762840</td>\n",
       "      <td>0.775316</td>\n",
       "      <td>0.740181</td>\n",
       "      <td>0.757342</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>245</td>\n",
       "      <td>266</td>\n",
       "      <td>65</td>\n",
       "      <td>86</td>\n",
       "      <td>0.771903</td>\n",
       "      <td>0.790323</td>\n",
       "      <td>0.740181</td>\n",
       "      <td>0.764431</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>256</td>\n",
       "      <td>257</td>\n",
       "      <td>74</td>\n",
       "      <td>75</td>\n",
       "      <td>0.774924</td>\n",
       "      <td>0.775758</td>\n",
       "      <td>0.773414</td>\n",
       "      <td>0.774584</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>247</td>\n",
       "      <td>259</td>\n",
       "      <td>72</td>\n",
       "      <td>84</td>\n",
       "      <td>0.764350</td>\n",
       "      <td>0.774295</td>\n",
       "      <td>0.746224</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>237</td>\n",
       "      <td>256</td>\n",
       "      <td>75</td>\n",
       "      <td>94</td>\n",
       "      <td>0.744713</td>\n",
       "      <td>0.759615</td>\n",
       "      <td>0.716012</td>\n",
       "      <td>0.737170</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>237</td>\n",
       "      <td>258</td>\n",
       "      <td>73</td>\n",
       "      <td>94</td>\n",
       "      <td>0.747734</td>\n",
       "      <td>0.764516</td>\n",
       "      <td>0.716012</td>\n",
       "      <td>0.739470</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>238</td>\n",
       "      <td>258</td>\n",
       "      <td>73</td>\n",
       "      <td>93</td>\n",
       "      <td>0.749245</td>\n",
       "      <td>0.765273</td>\n",
       "      <td>0.719033</td>\n",
       "      <td>0.741433</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>249</td>\n",
       "      <td>256</td>\n",
       "      <td>75</td>\n",
       "      <td>82</td>\n",
       "      <td>0.762840</td>\n",
       "      <td>0.768519</td>\n",
       "      <td>0.752266</td>\n",
       "      <td>0.760305</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>242</td>\n",
       "      <td>259</td>\n",
       "      <td>72</td>\n",
       "      <td>89</td>\n",
       "      <td>0.756798</td>\n",
       "      <td>0.770701</td>\n",
       "      <td>0.731118</td>\n",
       "      <td>0.750388</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>234</td>\n",
       "      <td>263</td>\n",
       "      <td>68</td>\n",
       "      <td>97</td>\n",
       "      <td>0.750755</td>\n",
       "      <td>0.774834</td>\n",
       "      <td>0.706949</td>\n",
       "      <td>0.739336</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>251</td>\n",
       "      <td>270</td>\n",
       "      <td>61</td>\n",
       "      <td>80</td>\n",
       "      <td>0.787009</td>\n",
       "      <td>0.804487</td>\n",
       "      <td>0.758308</td>\n",
       "      <td>0.780715</td>\n",
       "      <td>0.9</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>246</td>\n",
       "      <td>274</td>\n",
       "      <td>57</td>\n",
       "      <td>85</td>\n",
       "      <td>0.785498</td>\n",
       "      <td>0.811881</td>\n",
       "      <td>0.743202</td>\n",
       "      <td>0.776025</td>\n",
       "      <td>0.9</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>251</td>\n",
       "      <td>264</td>\n",
       "      <td>67</td>\n",
       "      <td>80</td>\n",
       "      <td>0.777946</td>\n",
       "      <td>0.789308</td>\n",
       "      <td>0.758308</td>\n",
       "      <td>0.773498</td>\n",
       "      <td>0.9</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>246</td>\n",
       "      <td>263</td>\n",
       "      <td>68</td>\n",
       "      <td>85</td>\n",
       "      <td>0.768882</td>\n",
       "      <td>0.783439</td>\n",
       "      <td>0.743202</td>\n",
       "      <td>0.762791</td>\n",
       "      <td>0.9</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>235</td>\n",
       "      <td>262</td>\n",
       "      <td>69</td>\n",
       "      <td>96</td>\n",
       "      <td>0.750755</td>\n",
       "      <td>0.773026</td>\n",
       "      <td>0.709970</td>\n",
       "      <td>0.740157</td>\n",
       "      <td>0.9</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>239</td>\n",
       "      <td>271</td>\n",
       "      <td>60</td>\n",
       "      <td>92</td>\n",
       "      <td>0.770393</td>\n",
       "      <td>0.799331</td>\n",
       "      <td>0.722054</td>\n",
       "      <td>0.758730</td>\n",
       "      <td>0.9</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>237</td>\n",
       "      <td>268</td>\n",
       "      <td>63</td>\n",
       "      <td>94</td>\n",
       "      <td>0.762840</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.716012</td>\n",
       "      <td>0.751189</td>\n",
       "      <td>0.9</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>241</td>\n",
       "      <td>268</td>\n",
       "      <td>63</td>\n",
       "      <td>90</td>\n",
       "      <td>0.768882</td>\n",
       "      <td>0.792763</td>\n",
       "      <td>0.728097</td>\n",
       "      <td>0.759055</td>\n",
       "      <td>0.9</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>237</td>\n",
       "      <td>265</td>\n",
       "      <td>66</td>\n",
       "      <td>94</td>\n",
       "      <td>0.758308</td>\n",
       "      <td>0.782178</td>\n",
       "      <td>0.716012</td>\n",
       "      <td>0.747634</td>\n",
       "      <td>0.9</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>232</td>\n",
       "      <td>268</td>\n",
       "      <td>63</td>\n",
       "      <td>99</td>\n",
       "      <td>0.755287</td>\n",
       "      <td>0.786441</td>\n",
       "      <td>0.700906</td>\n",
       "      <td>0.741214</td>\n",
       "      <td>0.9</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>253</td>\n",
       "      <td>267</td>\n",
       "      <td>64</td>\n",
       "      <td>78</td>\n",
       "      <td>0.785498</td>\n",
       "      <td>0.798107</td>\n",
       "      <td>0.764350</td>\n",
       "      <td>0.780864</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>248</td>\n",
       "      <td>273</td>\n",
       "      <td>58</td>\n",
       "      <td>83</td>\n",
       "      <td>0.787009</td>\n",
       "      <td>0.810458</td>\n",
       "      <td>0.749245</td>\n",
       "      <td>0.778650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>251</td>\n",
       "      <td>262</td>\n",
       "      <td>69</td>\n",
       "      <td>80</td>\n",
       "      <td>0.774924</td>\n",
       "      <td>0.784375</td>\n",
       "      <td>0.758308</td>\n",
       "      <td>0.771121</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>248</td>\n",
       "      <td>259</td>\n",
       "      <td>72</td>\n",
       "      <td>83</td>\n",
       "      <td>0.765861</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.749245</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>236</td>\n",
       "      <td>256</td>\n",
       "      <td>75</td>\n",
       "      <td>95</td>\n",
       "      <td>0.743202</td>\n",
       "      <td>0.758842</td>\n",
       "      <td>0.712991</td>\n",
       "      <td>0.735202</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>241</td>\n",
       "      <td>270</td>\n",
       "      <td>61</td>\n",
       "      <td>90</td>\n",
       "      <td>0.771903</td>\n",
       "      <td>0.798013</td>\n",
       "      <td>0.728097</td>\n",
       "      <td>0.761453</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>239</td>\n",
       "      <td>268</td>\n",
       "      <td>63</td>\n",
       "      <td>92</td>\n",
       "      <td>0.765861</td>\n",
       "      <td>0.791391</td>\n",
       "      <td>0.722054</td>\n",
       "      <td>0.755134</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>242</td>\n",
       "      <td>266</td>\n",
       "      <td>65</td>\n",
       "      <td>89</td>\n",
       "      <td>0.767372</td>\n",
       "      <td>0.788274</td>\n",
       "      <td>0.731118</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>237</td>\n",
       "      <td>264</td>\n",
       "      <td>67</td>\n",
       "      <td>94</td>\n",
       "      <td>0.756798</td>\n",
       "      <td>0.779605</td>\n",
       "      <td>0.716012</td>\n",
       "      <td>0.746457</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>232</td>\n",
       "      <td>266</td>\n",
       "      <td>65</td>\n",
       "      <td>99</td>\n",
       "      <td>0.752266</td>\n",
       "      <td>0.781145</td>\n",
       "      <td>0.700906</td>\n",
       "      <td>0.738854</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>251</td>\n",
       "      <td>271</td>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>0.788520</td>\n",
       "      <td>0.807074</td>\n",
       "      <td>0.758308</td>\n",
       "      <td>0.781931</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>246</td>\n",
       "      <td>274</td>\n",
       "      <td>57</td>\n",
       "      <td>85</td>\n",
       "      <td>0.785498</td>\n",
       "      <td>0.811881</td>\n",
       "      <td>0.743202</td>\n",
       "      <td>0.776025</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>251</td>\n",
       "      <td>264</td>\n",
       "      <td>67</td>\n",
       "      <td>80</td>\n",
       "      <td>0.777946</td>\n",
       "      <td>0.789308</td>\n",
       "      <td>0.758308</td>\n",
       "      <td>0.773498</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>245</td>\n",
       "      <td>263</td>\n",
       "      <td>68</td>\n",
       "      <td>86</td>\n",
       "      <td>0.767372</td>\n",
       "      <td>0.782748</td>\n",
       "      <td>0.740181</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>235</td>\n",
       "      <td>262</td>\n",
       "      <td>69</td>\n",
       "      <td>96</td>\n",
       "      <td>0.750755</td>\n",
       "      <td>0.773026</td>\n",
       "      <td>0.709970</td>\n",
       "      <td>0.740157</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>239</td>\n",
       "      <td>272</td>\n",
       "      <td>59</td>\n",
       "      <td>92</td>\n",
       "      <td>0.771903</td>\n",
       "      <td>0.802013</td>\n",
       "      <td>0.722054</td>\n",
       "      <td>0.759936</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>237</td>\n",
       "      <td>268</td>\n",
       "      <td>63</td>\n",
       "      <td>94</td>\n",
       "      <td>0.762840</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.716012</td>\n",
       "      <td>0.751189</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>241</td>\n",
       "      <td>271</td>\n",
       "      <td>60</td>\n",
       "      <td>90</td>\n",
       "      <td>0.773414</td>\n",
       "      <td>0.800664</td>\n",
       "      <td>0.728097</td>\n",
       "      <td>0.762658</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>237</td>\n",
       "      <td>267</td>\n",
       "      <td>64</td>\n",
       "      <td>94</td>\n",
       "      <td>0.761329</td>\n",
       "      <td>0.787375</td>\n",
       "      <td>0.716012</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>232</td>\n",
       "      <td>269</td>\n",
       "      <td>62</td>\n",
       "      <td>99</td>\n",
       "      <td>0.756798</td>\n",
       "      <td>0.789116</td>\n",
       "      <td>0.700906</td>\n",
       "      <td>0.742400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>220 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      TP   TN  FP   FN         A         P         R        F1  Alpha  Binary  \\\n",
       "0    183  267  64  148  0.679758  0.740891  0.552870  0.633218    0.0    True   \n",
       "1    224  259  72  107  0.729607  0.756757  0.676737  0.714514    0.0    True   \n",
       "2    259  242  89   72  0.756798  0.744253  0.782477  0.762887    0.0    True   \n",
       "3    249  253  78   82  0.758308  0.761468  0.752266  0.756839    0.0    True   \n",
       "4    237  251  80   94  0.737160  0.747634  0.716012  0.731481    0.0    True   \n",
       "5    177  258  73  154  0.657100  0.708000  0.534743  0.609294    0.0    True   \n",
       "6    220  257  74  111  0.720544  0.748299  0.664653  0.704000    0.0    True   \n",
       "7    251  243  88   80  0.746224  0.740413  0.758308  0.749254    0.0    True   \n",
       "8    243  253  78   88  0.749245  0.757009  0.734139  0.745399    0.0    True   \n",
       "9    234  257  74   97  0.741692  0.759740  0.706949  0.732394    0.0    True   \n",
       "10   182  268  63  149  0.679758  0.742857  0.549849  0.631944    0.0   False   \n",
       "11   224  259  72  107  0.729607  0.756757  0.676737  0.714514    0.0   False   \n",
       "12   259  243  88   72  0.758308  0.746398  0.782477  0.764012    0.0   False   \n",
       "13   248  255  76   83  0.759819  0.765432  0.749245  0.757252    0.0   False   \n",
       "14   236  253  78   95  0.738671  0.751592  0.712991  0.731783    0.0   False   \n",
       "15   177  259  72  154  0.658610  0.710843  0.534743  0.610345    0.0   False   \n",
       "16   221  259  72  110  0.725076  0.754266  0.667674  0.708333    0.0   False   \n",
       "17   250  246  85   81  0.749245  0.746269  0.755287  0.750751    0.0   False   \n",
       "18   242  257  74   89  0.753776  0.765823  0.731118  0.748068    0.0   False   \n",
       "19   232  260  71   99  0.743202  0.765677  0.700906  0.731861    0.0   False   \n",
       "20   245  260  71   86  0.762840  0.775316  0.740181  0.757342    0.1    True   \n",
       "21   245  266  65   86  0.771903  0.790323  0.740181  0.764431    0.1    True   \n",
       "22   256  257  74   75  0.774924  0.775758  0.773414  0.774584    0.1    True   \n",
       "23   247  259  72   84  0.764350  0.774295  0.746224  0.760000    0.1    True   \n",
       "24   237  256  75   94  0.744713  0.759615  0.716012  0.737170    0.1    True   \n",
       "25   237  258  73   94  0.747734  0.764516  0.716012  0.739470    0.1    True   \n",
       "26   238  258  73   93  0.749245  0.765273  0.719033  0.741433    0.1    True   \n",
       "27   249  256  75   82  0.762840  0.768519  0.752266  0.760305    0.1    True   \n",
       "28   242  259  72   89  0.756798  0.770701  0.731118  0.750388    0.1    True   \n",
       "29   234  263  68   97  0.750755  0.774834  0.706949  0.739336    0.1    True   \n",
       "..   ...  ...  ..  ...       ...       ...       ...       ...    ...     ...   \n",
       "190  251  270  61   80  0.787009  0.804487  0.758308  0.780715    0.9   False   \n",
       "191  246  274  57   85  0.785498  0.811881  0.743202  0.776025    0.9   False   \n",
       "192  251  264  67   80  0.777946  0.789308  0.758308  0.773498    0.9   False   \n",
       "193  246  263  68   85  0.768882  0.783439  0.743202  0.762791    0.9   False   \n",
       "194  235  262  69   96  0.750755  0.773026  0.709970  0.740157    0.9   False   \n",
       "195  239  271  60   92  0.770393  0.799331  0.722054  0.758730    0.9   False   \n",
       "196  237  268  63   94  0.762840  0.790000  0.716012  0.751189    0.9   False   \n",
       "197  241  268  63   90  0.768882  0.792763  0.728097  0.759055    0.9   False   \n",
       "198  237  265  66   94  0.758308  0.782178  0.716012  0.747634    0.9   False   \n",
       "199  232  268  63   99  0.755287  0.786441  0.700906  0.741214    0.9   False   \n",
       "200  253  267  64   78  0.785498  0.798107  0.764350  0.780864    1.0    True   \n",
       "201  248  273  58   83  0.787009  0.810458  0.749245  0.778650    1.0    True   \n",
       "202  251  262  69   80  0.774924  0.784375  0.758308  0.771121    1.0    True   \n",
       "203  248  259  72   83  0.765861  0.775000  0.749245  0.761905    1.0    True   \n",
       "204  236  256  75   95  0.743202  0.758842  0.712991  0.735202    1.0    True   \n",
       "205  241  270  61   90  0.771903  0.798013  0.728097  0.761453    1.0    True   \n",
       "206  239  268  63   92  0.765861  0.791391  0.722054  0.755134    1.0    True   \n",
       "207  242  266  65   89  0.767372  0.788274  0.731118  0.758621    1.0    True   \n",
       "208  237  264  67   94  0.756798  0.779605  0.716012  0.746457    1.0    True   \n",
       "209  232  266  65   99  0.752266  0.781145  0.700906  0.738854    1.0    True   \n",
       "210  251  271  60   80  0.788520  0.807074  0.758308  0.781931    1.0   False   \n",
       "211  246  274  57   85  0.785498  0.811881  0.743202  0.776025    1.0   False   \n",
       "212  251  264  67   80  0.777946  0.789308  0.758308  0.773498    1.0   False   \n",
       "213  245  263  68   86  0.767372  0.782748  0.740181  0.760870    1.0   False   \n",
       "214  235  262  69   96  0.750755  0.773026  0.709970  0.740157    1.0   False   \n",
       "215  239  272  59   92  0.771903  0.802013  0.722054  0.759936    1.0   False   \n",
       "216  237  268  63   94  0.762840  0.790000  0.716012  0.751189    1.0   False   \n",
       "217  241  271  60   90  0.773414  0.800664  0.728097  0.762658    1.0   False   \n",
       "218  237  267  64   94  0.761329  0.787375  0.716012  0.750000    1.0   False   \n",
       "219  232  269  62   99  0.756798  0.789116  0.700906  0.742400    1.0   False   \n",
       "\n",
       "     Mark Negation  Min Frequency  \n",
       "0             True              1  \n",
       "1             True              2  \n",
       "2             True              3  \n",
       "3             True              4  \n",
       "4             True              5  \n",
       "5            False              1  \n",
       "6            False              2  \n",
       "7            False              3  \n",
       "8            False              4  \n",
       "9            False              5  \n",
       "10            True              1  \n",
       "11            True              2  \n",
       "12            True              3  \n",
       "13            True              4  \n",
       "14            True              5  \n",
       "15           False              1  \n",
       "16           False              2  \n",
       "17           False              3  \n",
       "18           False              4  \n",
       "19           False              5  \n",
       "20            True              1  \n",
       "21            True              2  \n",
       "22            True              3  \n",
       "23            True              4  \n",
       "24            True              5  \n",
       "25           False              1  \n",
       "26           False              2  \n",
       "27           False              3  \n",
       "28           False              4  \n",
       "29           False              5  \n",
       "..             ...            ...  \n",
       "190           True              1  \n",
       "191           True              2  \n",
       "192           True              3  \n",
       "193           True              4  \n",
       "194           True              5  \n",
       "195          False              1  \n",
       "196          False              2  \n",
       "197          False              3  \n",
       "198          False              4  \n",
       "199          False              5  \n",
       "200           True              1  \n",
       "201           True              2  \n",
       "202           True              3  \n",
       "203           True              4  \n",
       "204           True              5  \n",
       "205          False              1  \n",
       "206          False              2  \n",
       "207          False              3  \n",
       "208          False              4  \n",
       "209          False              5  \n",
       "210           True              1  \n",
       "211           True              2  \n",
       "212           True              3  \n",
       "213           True              4  \n",
       "214           True              5  \n",
       "215          False              1  \n",
       "216          False              2  \n",
       "217          False              3  \n",
       "218          False              4  \n",
       "219          False              5  \n",
       "\n",
       "[220 rows x 12 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dev set results, test is below\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# initial list of values\n",
    "values = []\n",
    "\n",
    "# loop over the results\n",
    "for row in results:\n",
    "    \n",
    "    # create a new list\n",
    "    dictlist = []\n",
    "    \n",
    "    # loop over the values in the dictionary of one row\n",
    "    for value in row.values():\n",
    "        \n",
    "        # add the value to the list\n",
    "        dictlist.append(value)\n",
    "    \n",
    "    # add the converted dict (now a list) to the values list as a row\n",
    "    values.append(dictlist)\n",
    "    \n",
    "# get the keys\n",
    "keys = [x for x in results[0].keys()] \n",
    "    \n",
    "# create the pandas dataframe (can be printed as a table)\n",
    "output = pd.DataFrame(values, columns=keys)\n",
    "\n",
    "# print the dataframe as table\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features:\n",
      " 1882 features\n",
      "MLE: counting\n",
      "MLE: smoothing\n",
      "MLE: done\n"
     ]
    }
   ],
   "source": [
    "# We have chosen the F1-score since precision and recall are both important for a good model\n",
    "# and the F1-score is an evaluation of both\n",
    "best_classifier = NaiveBayesClassifier(\n",
    "    test_pos, test_neg, \n",
    "    binary=best_F1['Binary'], mark_negation=best_F1['Mark Negation'],\n",
    "    alpha=best_F1['Alpha'], min_freq=best_F1['Min Frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table of test results\n",
      "-------------------------\n",
      "TP \t| 237\n",
      "TN \t| 235\n",
      "FP \t| 96\n",
      "FN \t| 94\n",
      "A \t| 0.7129909365558912\n",
      "P \t| 0.7117117117117117\n",
      "R \t| 0.716012084592145\n",
      "F1 \t| 0.713855421686747\n"
     ]
    }
   ],
   "source": [
    "# get the test metrics\n",
    "test_metrics = evaluate_model(best_classifier, dev_pos, dev_neg)\n",
    "\n",
    "# print the table\n",
    "print('Table of test results')\n",
    "print('-------------------------')\n",
    "for key, value in test_metrics.items():\n",
    "    print(key, \"\\t|\", value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

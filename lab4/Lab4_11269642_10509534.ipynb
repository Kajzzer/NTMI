{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lotte Bottema 11269642\n",
    "## Kaj Meijer 10509534\n",
    "\n",
    "**Table of contents**\n",
    "\n",
    "* [Parts of Speech](#pos)\n",
    "* [Hidden Markov Models](#hmm)\n",
    "* [Maximum likelihood estimation for labelled data](#mle)\n",
    "* [Implementation](#imp)\n",
    "* [Evaluation](#eval)\n",
    "    * [Perplexity](#ppl)\n",
    "    * [Viterbi](#viterbi)    \n",
    "    * [Accuracy](#acc)\n",
    "\n",
    "    \n",
    "**Table of Exercises**\n",
    "\n",
    "\n",
    "* [Exercise 4-1](#ex4-1) (-/2)\n",
    "* [Exercise 4-2](#ex4-2) (-/1)\n",
    "* [Exercise 4-3](#ex4-3) (-/1)\n",
    "* [Exercise 4-4](#ex4-4) (-/1)\n",
    "* [Exercise 4-5](#ex4-5) (-/2)\n",
    "* [Exercise 4-6](#ex4-6) (-/1)\n",
    "* [Exercise 4-7](#ex4-7) (-/2)\n",
    "* [Exercise 4-8](#ex4-8) (-/3)\n",
    "* [Exercise 4-9](#ex4-9) (-/1)\n",
    "* [Exercise 4-10](#ex4-10) (-/17)\n",
    "* [Exercise 4-11](#ex4-11) (-/4)\n",
    "* [Exercise 4-12](#ex4-12) (-/5)\n",
    "\n",
    "\n",
    "\n",
    "**General notes**\n",
    "\n",
    "* In this notebook you are expected to use $\\LaTeX$. \n",
    "* Use python3.\n",
    "* Use NLTK to read annotated data.\n",
    "* **Document your code**: TAs are more likely to understand the steps if you document them. If you don't, it's also difficult to give you partial points for exercises that are not completely correct.\n",
    "\n",
    "After completing this lab you should be able to \n",
    "\n",
    "* develop HMM taggers and language models\n",
    "* estimate parameters via MLE\n",
    "* marginalise latent variables and evaluate the model intrinsically in terms of perplexity\n",
    "* predict the best POS tag sequence and evaluate the model in terms of accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"pos\"> Parts of Speech\n",
    "    \n",
    "**Parts of speech** (also known as PoS, word classes) give us information about a word and its neighbors. \n",
    "PoS can can be divided into: closed class and open class.\n",
    "\n",
    "Open class words (or **content words**) are nouns, verbs, adjectives, and adverbs, where they refer to objects, actions, and features in the world. They are called open class, since there is no limit to what these words are\n",
    "new ones are added all the time (email, website, selfie, etc.).\n",
    "\n",
    "**Nouns** suchs as proper nouns are names of persons of entitnes: *Regina*, *Colorado*,\n",
    "and *IBM*. Other type are common nouns that refer to objects that for exmaple, can be counted (one car) or homogeneus groups (snow, sand).\n",
    "\n",
    "**Verbs** consists pf actions and processes, like *draw*, *provide*, and *go*.\n",
    "\n",
    "***Adjectives** include terms for properties or qualities for concepts like age (*old*, *young*), value (*good*, *bad*).\n",
    "\n",
    "\n",
    "\n",
    "Closed class words (or **function words**) are pronouns, determiners, prepositions, and connectives. There is a limited number of these.\n",
    "\n",
    "\n",
    "**Prepositions** occur before noun phrases and indicate spatial or temporal relations, for example, *by* the house.\n",
    "\n",
    "The PoS are tags that classifiy words. For example in English uses the 36 tags. And these tags are used to manually annotate a wide variety of corpora, the Brown corpus, the Wall Street Journal corpus.\n",
    "\n",
    "In this Lab we are going to work with the English [Penn Treebank](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) tag set and the annotated corpus.\n",
    "\n",
    "Our final goal is the task: \n",
    "* **Part-of-speech tagging** (tagging for short) is the process of assigning a part-ofspeech tag to each word in an input text.\n",
    "\n",
    "First, we will download the annotated data from [NLTK](https://www.nltk.org/data.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read annotated corpora with NLTK\n",
    "# first download data\n",
    "import nltk\n",
    "# nltk.download()\n",
    "# it will open a GUI and you have to double click in \"all\" to download \n",
    "# this will download different types of annotated corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With NLTK, we load the annotated **Penn Treebank** corpus.\n",
    "This corpus will be used to train and test our PoS taggers. Let's use the last 100 sentences for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3914\n",
      "3000\n",
      "100\n",
      "814\n"
     ]
    }
   ],
   "source": [
    "# inspect PoS from Treebank\n",
    "# we use the universal tagset\n",
    "treebank_sents = nltk.corpus.treebank.tagged_sents(tagset='universal')\n",
    "\n",
    "# we split our corpus on training, dev and test\n",
    "treebank_training = list(treebank_sents[:3000]) \n",
    "treebank_dev = list(treebank_sents[3000:3100])\n",
    "treebank_test = list(treebank_sents[3100:])\n",
    "print(len(treebank_sents))\n",
    "print(len(treebank_training)) # if it takes too long/memory only use 1000 instances for training!\n",
    "print(len(treebank_dev)) # we use 100 sentences/instances for validation\n",
    "print(len(treebank_test)) # we use 814 sentences/instances for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12408\n",
      "{'ADJ', 'CONJ', 'ADP', 'PRON', 'X', 'ADV', 'NOUN', '.', 'DET', 'PRT', 'VERB', 'NUM'}\n"
     ]
    }
   ],
   "source": [
    "# which is the vocabulary?\n",
    "# we can inspect the vocabulary of the corpus \n",
    "vocabulary = set([w for (w, t) in nltk.corpus.treebank.tagged_words(tagset='universal')])\n",
    "print(len(vocabulary)) # number of words in our corpus\n",
    "# we inspect the universal tagset (this is a general mapping becasue each languge use a different tagset.\n",
    "tagset = set([t for (w, t) in nltk.corpus.treebank.tagged_words(tagset='universal')])\n",
    "\n",
    "print(tagset) # tags/labes used to annotate the word class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The observations are pairs: (sentence, tags)\n",
    "# so we can use this to get just the sentences (when we need them)\n",
    "def extract_sentences(treebank_corpus):\n",
    "    sentences = []\n",
    "    for observations in treebank_corpus:\n",
    "        sentences.append([x for x, c in observations])\n",
    "    return sentences\n",
    "\n",
    "# The observations are pairs: (sentence, tags)\n",
    "# so we can use this to get just the tags (when we need them)\n",
    "def extract_tags(treebank_corpus):\n",
    "    tags = []\n",
    "    for observations in treebank_corpus:\n",
    "        tags.append([c for x, c in observations])\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex4-1\" style=\"color:red\">**Exercise 4-1**</a> **[2 points]** \n",
    "\n",
    "* **[1 point]** Load the PTB corpus and plot the distribution of POS tags. \n",
    "* **[1 point]** Do the same for the Brown corpus.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEICAYAAAC0+DhzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAGFxJREFUeJzt3X20XXV95/H3p0EcLLWgpIiQZVBjFaxFzCAddYpPEHDa4BqWQqcSXIyxFaYyakdwZhZYtVWn1FmMioNDCnSsgfpQGAQxIm11KkKQCASkBERJBiESHsqSqtDv/LF/V7d335t77kNyA3m/1jrr7v3dv/3be59z7v2c/XD2TVUhSVLfL8z3CkiSdjyGgyRpwHCQJA0YDpKkAcNBkjRgOEiSBgwHSdKA4SCNKMmdSR5J8nCSe5Kcl+T2Nv5wkseS/FNv/D1JTmj1sdodSX5/vrdFmorhIE3Pb1XV7sDBwFLgr6pq91b7KnDy2HhV/XGb5+u9Nv8W+HCSF8/P6kujMRykGaiqTcDlwAunOd/1wC3AC7bFeklzxXCQZiDJIuAo4PppzvcvgecBa7fFeklzZZf5XgHpceavkzwKPAh8AfjjKdoDHJrkAWABsDvwUeC2bbeK0uy55yBNz9FVtUdVPauq3lZVj4wwz9Vtnl8CngEcyGihIs0bw0HajqrqHuCzwG/N97pIW2M4SNtRkqcDrwfWz/e6SFtjOEjb3m+Mfc+B7kqlzcB/mOd1krYq/rMfSdJ47jlIkgYMB0nSgOEgSRqYMhyS/Isk1yT5VpL1Sd7b6vsn+UaSDUkuTLJrqz+5jW9o0xf3+jqt1W9NckSvvqzVNiQ5de43U5I0HVOekE4S4Ber6uEkTwK+BrwdeAfwuapaneQTwLeq6uwkbwNeVFW/l+RY4PVV9cYkBwCfBg4Bngl8me42AgD/ALwW2AhcCxxXVTdvbb322muvWrx48cy2WpJ2Utddd90PqmrhVO2mvH1GdenxcBt9UnsU8Crgd1r9fOAM4GxgeRsG+Azw0RYwy4HVVfUj4DtJNtAFBcCGqroDIMnq1nar4bB48WLWrvX2NJI0HUm+O0q7kc45JFmQZB1wL7AGuB14oKoebU02Avu24X2BuwDa9AeBp/fr4+aZrC5JmicjhUNVPVZVBwH70X3af/42XatJJFmZZG2StZs3b56PVZCkncK0rlaqqgeAq4DfAPZIMnZYaj9gUxveBCwCaNN/GbivXx83z2T1iZZ/TlUtraqlCxdOechMkjRDo1yttDDJHm14N7oTx7fQhcQxrdkK4OI2fEkbp03/SjtvcQlwbLuaaX9gCXAN3QnoJe3qp12BY1tbSdI8GeX/OewDnJ9kAV2YXFRVlya5GVid5P10//Dk3Nb+XOAv2gnnLXR/7Kmq9UkuojvR/ChwUlU9BpDkZOAKuvvdr6oqb0omSfPocXtvpaVLl5ZXK0nS9CS5rqqWTtXOb0hLkgYMB0nSgOEgSRoY5YT0E87iU78w533e+cHXzXmfkjRf3HOQJA0YDpKkAcNBkjRgOEiSBgwHSdKA4SBJGjAcJEkDhoMkacBwkCQNGA6SpAHDQZI0YDhIkgYMB0nSgOEgSRowHCRJA4aDJGnAcJAkDRgOkqQBw0GSNGA4SJIGDAdJ0oDhIEkaMBwkSQNThkOSRUmuSnJzkvVJ3t7qZyTZlGRdexzVm+e0JBuS3JrkiF59WattSHJqr75/km+0+oVJdp3rDZUkjW6UPYdHgXdW1QHAocBJSQ5o0z5SVQe1x2UAbdqxwIHAMuDjSRYkWQB8DDgSOAA4rtfPh1pfzwXuB06co+2TJM3AlOFQVXdX1Tfb8D8CtwD7bmWW5cDqqvpRVX0H2AAc0h4bquqOqvoxsBpYniTAq4DPtPnPB46e6QZJkmZvWucckiwGXgx8o5VOTnJDklVJ9my1fYG7erNtbLXJ6k8HHqiqR8fVJ1r+yiRrk6zdvHnzdFZdkjQNI4dDkt2BzwKnVNVDwNnAc4CDgLuBM7fJGvZU1TlVtbSqli5cuHBbL06Sdlq7jNIoyZPoguFTVfU5gKq6pzf9k8ClbXQTsKg3+36txiT1+4A9kuzS9h767SVJ82CUq5UCnAvcUlV/1qvv02v2euCmNnwJcGySJyfZH1gCXANcCyxpVybtSnfS+pKqKuAq4Jg2/wrg4tltliRpNkbZc3gZ8CbgxiTrWu09dFcbHQQUcCfwVoCqWp/kIuBmuiudTqqqxwCSnAxcASwAVlXV+tbfu4HVSd4PXE8XRpKkeTJlOFTV14BMMOmyrczzAeADE9Qvm2i+qrqD7momSdIOwG9IS5IGDAdJ0oDhIEkaMBwkSQOGgyRpwHCQJA0YDpKkAcNBkjRgOEiSBgwHSdKA4SBJGjAcJEkDhoMkacBwkCQNGA6SpAHDQZI0YDhIkgYMB0nSgOEgSRowHCRJA4aDJGnAcJAkDRgOkqQBw0GSNGA4SJIGDAdJ0oDhIEkamDIckixKclWSm5OsT/L2Vn9akjVJbms/92z1JDkryYYkNyQ5uNfXitb+tiQrevWXJLmxzXNWkmyLjZUkjWaUPYdHgXdW1QHAocBJSQ4ATgWurKolwJVtHOBIYEl7rATOhi5MgNOBlwKHAKePBUpr85befMtmv2mSpJmaMhyq6u6q+mYb/kfgFmBfYDlwfmt2PnB0G14OXFCdq4E9kuwDHAGsqaotVXU/sAZY1qY9taqurqoCLuj1JUmaB9M655BkMfBi4BvA3lV1d5v0fWDvNrwvcFdvto2ttrX6xgnqEy1/ZZK1SdZu3rx5OqsuSZqGkcMhye7AZ4FTquqh/rT2ib/meN0GquqcqlpaVUsXLly4rRcnSTutkcIhyZPoguFTVfW5Vr6nHRKi/by31TcBi3qz79dqW6vvN0FdkjRPRrlaKcC5wC1V9We9SZcAY1ccrQAu7tWPb1ctHQo82A4/XQEcnmTPdiL6cOCKNu2hJIe2ZR3f60uSNA92GaHNy4A3ATcmWddq7wE+CFyU5ETgu8Ab2rTLgKOADcAPgTcDVNWWJO8Drm3t/qiqtrThtwHnAbsBl7eHJGmeTBkOVfU1YLLvHbx6gvYFnDRJX6uAVRPU1wIvnGpdJEnbh9+QliQNGA6SpAHDQZI0YDhIkgYMB0nSgOEgSRowHCRJA4aDJGnAcJAkDRgOkqQBw0GSNGA4SJIGDAdJ0oDhIEkaMBwkSQOGgyRpwHCQJA0YDpKkAcNBkjRgOEiSBgwHSdKA4SBJGjAcJEkDhoMkacBwkCQNGA6SpAHDQZI0MGU4JFmV5N4kN/VqZyTZlGRdexzVm3Zakg1Jbk1yRK++rNU2JDm1V98/yTda/cIku87lBkqSpm+UPYfzgGUT1D9SVQe1x2UASQ4AjgUObPN8PMmCJAuAjwFHAgcAx7W2AB9qfT0XuB84cTYbJEmavSnDoar+DtgyYn/LgdVV9aOq+g6wATikPTZU1R1V9WNgNbA8SYBXAZ9p858PHD3NbZAkzbHZnHM4OckN7bDTnq22L3BXr83GVpus/nTggap6dFx9QklWJlmbZO3mzZtnseqSpK2ZaTicDTwHOAi4GzhzztZoK6rqnKpaWlVLFy5cuD0WKUk7pV1mMlNV3TM2nOSTwKVtdBOwqNd0v1Zjkvp9wB5Jdml7D/32kqR5MqM9hyT79EZfD4xdyXQJcGySJyfZH1gCXANcCyxpVybtSnfS+pKqKuAq4Jg2/wrg4pmskyRp7ky555Dk08BhwF5JNgKnA4clOQgo4E7grQBVtT7JRcDNwKPASVX1WOvnZOAKYAGwqqrWt0W8G1id5P3A9cC5c7Z1kqQZmTIcquq4CcqT/gGvqg8AH5igfhlw2QT1O+iuZpIk7SD8hrQkacBwkCQNGA6SpAHDQZI0MKPvOUjbyuJTvzDnfd75wdfNeZ/SE517DpKkAcNBkjRgOEiSBgwHSdKA4SBJGjAcJEkDhoMkacBwkCQNGA6SpAHDQZI0YDhIkgYMB0nSgOEgSRrwrqzb0FzfYdS7i0raXtxzkCQNGA6SpAHDQZI0YDhIkgYMB0nSgOEgSRowHCRJA4aDJGlgynBIsirJvUlu6tWelmRNktvazz1bPUnOSrIhyQ1JDu7Ns6K1vy3Jil79JUlubPOclSRzvZGSpOkZZc/hPGDZuNqpwJVVtQS4so0DHAksaY+VwNnQhQlwOvBS4BDg9LFAaW3e0ptv/LIkSdvZlOFQVX8HbBlXXg6c34bPB47u1S+oztXAHkn2AY4A1lTVlqq6H1gDLGvTnlpVV1dVARf0+pIkzZOZnnPYu6rubsPfB/Zuw/sCd/XabWy1rdU3TlCfUJKVSdYmWbt58+YZrrokaSqzvvFeVVWSmouVGWFZ5wDnACxdunS7LFOaDW++qMerme453NMOCdF+3tvqm4BFvXb7tdrW6vtNUJckzaOZhsMlwNgVRyuAi3v149tVS4cCD7bDT1cAhyfZs52IPhy4ok17KMmh7Sql43t9SZLmyZSHlZJ8GjgM2CvJRrqrjj4IXJTkROC7wBta88uAo4ANwA+BNwNU1ZYk7wOube3+qKrGTnK/je6KqN2Ay9tDkjSPpgyHqjpukkmvnqBtASdN0s8qYNUE9bXAC6daD0nS9uM3pCVJA4aDJGnAcJAkDcz6ew7S49Fcf/8A/A6Cnljcc5AkDRgOkqQBw0GSNGA4SJIGDAdJ0oDhIEkaMBwkSQOGgyRpwHCQJA0YDpKkAcNBkjRgOEiSBgwHSdKA4SBJGvCW3ZJG4m3Ody7uOUiSBtxz0Ej81CjtXNxzkCQNGA6SpAHDQZI0YDhIkgYMB0nSgOEgSRqYVTgkuTPJjUnWJVnbak9LsibJbe3nnq2eJGcl2ZDkhiQH9/pZ0drflmTF7DZJkjRbc7Hn8MqqOqiqlrbxU4Erq2oJcGUbBzgSWNIeK4GzoQsT4HTgpcAhwOljgSJJmh/b4rDScuD8Nnw+cHSvfkF1rgb2SLIPcASwpqq2VNX9wBpg2TZYL0nSiGYbDgV8Kcl1SVa22t5VdXcb/j6wdxveF7irN+/GVpusPpBkZZK1SdZu3rx5lqsuSZrMbG+f8fKq2pTkV4A1Sb7dn1hVlaRmuYx+f+cA5wAsXbp0zvqVJP28We05VNWm9vNe4PN05wzuaYeLaD/vbc03AYt6s+/XapPVJUnzZMbhkOQXk/zS2DBwOHATcAkwdsXRCuDiNnwJcHy7aulQ4MF2+OkK4PAke7YT0Ye3miRpnszmsNLewOeTjPXzl1X1xSTXAhclORH4LvCG1v4y4ChgA/BD4M0AVbUlyfuAa1u7P6qqLbNYL0nSLM04HKrqDuDXJ6jfB7x6gnoBJ03S1ypg1UzXRZI0t/yGtCRpwHCQJA0YDpKkAcNBkjTg/5B+Apjr/+/s/3aW5J6DJGnAcJAkDXhYSdIOZa4Pk8LEh0o9HLt17jlIkgYMB0nSgIeVJGkberwevnLPQZI0YDhIkgYMB0nSgOccpMe57XXpp3Yu7jlIkgYMB0nSgOEgSRowHCRJA4aDJGnAcJAkDRgOkqQBw0GSNGA4SJIGDAdJ0oDhIEkaMBwkSQM7TDgkWZbk1iQbkpw63+sjSTuzHSIckiwAPgYcCRwAHJfkgPldK0naee0Q4QAcAmyoqjuq6sfAamD5PK+TJO20UlXzvQ4kOQZYVlX/vo2/CXhpVZ08rt1KYGUb/VXg1m28ansBP9jGy3A5O+4yXM6OvZwn0rZsz+U8q6oWTtXocfXPfqrqHOCc7bW8JGuraqnL2fGW80TaFpez4y7jibicUe0oh5U2AYt64/u1miRpHuwo4XAtsCTJ/kl2BY4FLpnndZKkndYOcVipqh5NcjJwBbAAWFVV6+d5tWD7HcJyOTvmMlzOjr2cJ9K2bM/ljGSHOCEtSdqx7CiHlSRJOxDDQZI0sFOEQ5JKcmZv/F1JzuiNr0zy7fa4JsnLe9PuTLJXb/ywJJe24ROS/HOSF/Wm35Rk8TbepJ/a2rYlOa99h6Tf/uH2c3Gb9/29aXsl+UmSj46w3KPb/M/v9fdIkuuT3NKexxN67U+YrN8kVyU5YlztlCSXtz7X9R7Ht+l3JrkxyQ1J/jbJs3rzPtbafivJN5P8q0mWO9ZufWv7ziS/0KYdluTBcct+Y2/4+0k29cZ33QbP1+bW981J3rKVvp+RZHWS25Ncl+SyJM9LcmCSr7Tb0tyW5L8mSa//Sd+749/3U0myKMl3kjytje/ZxheP2scEfY78nLVpG8dev14f65K8dIrljL0PbkryV0meMkH9/yTZI8mv9V7zLW0b1yX58kz77M0z49drm6iqJ/wD+CfgO8BebfxdwBlt+N8A1/WmHQx8D3hGG79zbFobPwy4tA2f0Npe2Jt+E7B4B9m284BjxrV/uP1cDNwBXN+b9vvAOuCjIyz3QuCrwHt7/d3Um/7s1tebe8/VhP3SfbHxz8fVrgb+db/PcdN/+roA7wU+OX4b2/ARwN9O0ke/3a8AX+5tz09f50nmPQN41zRepxk/X23dNgN7T9BvgK8Dv9er/TrwCuB24PBWewpwOXDSKO/d8e/7EbfxPwHntOH/CZw2y/f2dJ+zvwd+szf9+cDtIyyn/z74FPCOCernA/953HznMe73a6Z9ArvN5vXaFo+dYs8BeJTuSoD/OMG0dwN/WFU/AKiqb9K9aCeN2PelwIFJfnUuVnQGtrZtU/khcEuSsS/evBG4aKqZkuwOvBw4ke6y44GqugN4B/AHI6zHZ4DXjX36bp+GngncNcK80P1x3HeSaU8F7p+qg6q6ly6kTh77tDZXZvt8tXW7HXjW+GnAK4GfVNUneu2/BTwP+L9V9aVW+yFwMtC/qeVcv3c/Ahya5BS67f3TmXY0w+fs0+PaHkt3K57p+Crw3AnqW3uPzUWfv8P2f722amcJB+hu7PfvkvzyuPqBdHsOfWtbfRT/DHwYeM/sVm9WJtu2UawGjk2yCHgM+H8jzLMc+GJV/QNwX5KXTNLum3Sf3raqqrYA19DdeBG6X+qLgAKeM+7Qzism6GIZ8Ne98d1a228D/wt43wjbNPbHZgHdJ3WAV4xb9nNG6WcCs3q+kjyb7lPyhgnmeSHD9y9M8L6uqtuB3ZM8tZXm9L1bVT8B/pAuJE5p4zM1k+fsIuDoJGOX6L+RLjBG0uY7ErhxXH0B8Gpm8N2rafS53V+vqew04VBVDwEXMNon2Z+bdYTaX9J9Ytp/Jus2W1vZtlHW/YvAa+n+IF844iKP42efyFa38YlM5xN4/1Pfsfzsl/r2qjqo9/hqb56rkmyi++Xr/xF4pLV9Pl1wXDDDvYGvjlv27TPoA2b+fL0xyTq6bXtrC9G5Ntfv3SOBu+lCazam/ZxV1T10h1peneQg4NGqummEZe3Wnue1dIduzh1X/z6wN7BmGuu/LfqE7fi3Zof4Etx29N/pPmn8ea92M/AS4Cu92kuAsS/h3Qfsyc9uiPU0xt0cq7ov8Z1Jd4hqvky0bWPrDkA7WTh+3X+c5DrgnXS3S//trS2k9fEq4NeSFN0n7aLbexnvxcAtI67/xcBHkhwMPKWqrhvhZNsrgQfojum+l+4Qw8+pqq+3E6sLgXu31ln7hP5Ya/eCEdd7q2b5fF1Y424+OYH1wDET1G+mO2fTX5dn0x3zfmgsK+fyvdv+IL8WOBT4WpLVVXX3DPqZzXM29iHjHkbfa3ikqg6arN5OJl9Bd6j5rG3U53Z/vaay0+w5wE8PX1xEdxxzzIeBDyV5Ovz0DX4C8PE2/W+AN7VpC4DfBa6aoPvzgNfQ/RHa7ibZtr+h+/Q5diXNCUy87mcC7x7xk+kxwF9U1bOqanFVLaI7Id6/N9bYeYM/Bf7HiOv/cFu3VUzjUEBVPQqcAhzf/qj8nHalywK6oJxUkoXAJ+hOAs/lN0O3yfPV8xXgyenuWDzW14vo7lj88iSvabXd6P4IfXiCPs5jlu/dtmd2Nt3hpO8B/42Zn3OYzXP2OeAoukNK0z3fMKF2/P8PgHf2DlnNdZ+fYju+XqPYqcKhOZPu1rgAVNUldH+Q/r4do/4k8Lu9TzzvA56b5FvA9XTHff/3+E6r+z8UZ/Gz49UjS3fp4TOnO98Exm/bpXQnw65ru7IvY4JPHFW1vqrOH3EZxwGfH1f7LHAa3fmB65PcQhdUZ1XV2J7MLsCPpuj703RX2vTDYfw5h4lO2N7d5hm7iGDsnMM6ukNlK6rqsQmWN9ZuPd2VSl+i2wMZM/6cw0Sf0Kcy0+drJC3IXg+8Jt2lrOuBP6E7bLEc+C9JbqU75n0tMLiceJL37iivV99bgO9V1dhhko8DL0jym9PZnmbGz1lVPUB3oveedg5pTlTV9cANTH54a1Z9VtUjzO71mnPePkPbRZKPALdV1cenbKx51fai1lXVTK/O0RPAzrjnoO0syeXAi+h2nbUDS/LbdHubp833umh+uecgSRpwz0GSNGA4SJIGDAdJ0oDhIEkaMBwkSQP/H71sEbXS0vTsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def count_tags(tags):\n",
    "    count = defaultdict(int)\n",
    "    for sentence in tags:\n",
    "        for tag in sentence:\n",
    "            count[tag] += 1\n",
    "    return count\n",
    "\n",
    "ptb_corpus = nltk.corpus.treebank.tagged_sents(tagset='universal')\n",
    "brown_corpus = nltk.corpus.brown.tagged_sents(tagset='universal')\n",
    "\n",
    "ptb_tags = extract_tags(ptb_corpus)\n",
    "brown_tags = extract_tags(brown_corpus)\n",
    "\n",
    "ptb_tag_count = count_tags(ptb_tags)\n",
    "brown_tag_count = count_tags(brown_tags)\n",
    "\n",
    "plt.bar(range(len(ptb_tag_count)), ptb_tag_count.values(), align='center')\n",
    "plt.xticks(range(len(ptb_tag_count)), ptb_tag_count.keys())\n",
    "plt.title(\"PTB\")\n",
    "plt.show()\n",
    "\n",
    "plt.bar(range(len(brown_tag_count)), brown_tag_count.values(), align='center')\n",
    "plt.xticks(range(len(brown_tag_count)), brown_tag_count.keys())\n",
    "plt.title(\"Brown\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex4-2\" style=\"color:red\">**Exercise 4-2**</a> **[1 point]** Load the PTB corpus, find out for each POS tag what is the most likely tag that follows it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('BoS',) is followed the most by NOUN , 1141 times\n",
      "('NOUN',) is followed the most by NOUN , 7623 times\n",
      "('.',) is followed the most by EoS , 3884 times\n",
      "('NUM',) is followed the most by NOUN , 1252 times\n",
      "('ADJ',) is followed the most by NOUN , 4474 times\n",
      "('VERB',) is followed the most by X , 2954 times\n",
      "('DET',) is followed the most by NOUN , 5569 times\n",
      "('ADP',) is followed the most by DET , 3194 times\n",
      "('CONJ',) is followed the most by NOUN , 792 times\n",
      "('X',) is followed the most by VERB , 1354 times\n",
      "('ADV',) is followed the most by VERB , 1093 times\n",
      "('PRT',) is followed the most by VERB , 1291 times\n",
      "('PRON',) is followed the most by VERB , 1329 times\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def preprocess_history(history, order):\n",
    "    \"\"\"\n",
    "    This function pre-process an arbitrary history to match the order of this language model.\n",
    "    :param history: a sequence of words\n",
    "    :return: a tuple containing exactly as many elements as the order of the model\n",
    "        - if the input history is too short we pad it with <s> \n",
    "    \"\"\"\n",
    "    if len(history) == order:\n",
    "        return tuple(history)\n",
    "    elif len(history) > order:\n",
    "        length = len(history)            \n",
    "        return tuple(history[length - order: length]) #NOTE: we fixed the bug!\n",
    "    else:  # here the history is too short\n",
    "        missing = order - len(history)\n",
    "        return tuple(['BoS'] * missing) + tuple(history)\n",
    "\n",
    "\n",
    "# give the Markov Model order\n",
    "def count_ngrams(corpus, order):\n",
    "    \n",
    "    # the dict with histories and dicts\n",
    "    result = dict()\n",
    "    \n",
    "    # loop over the sentences in the corpus\n",
    "    for sentence in corpus:\n",
    "        \n",
    "        # add the EOS token to the sentence\n",
    "        sentence = sentence.copy() + ['EoS']\n",
    "        \n",
    "        # reset the history\n",
    "        history = []\n",
    "        \n",
    "        # loop over the tokens in the sentence\n",
    "        for token in sentence:\n",
    "            \n",
    "            # defines the key which is a tuple of the history\n",
    "            key = preprocess_history(history, order)\n",
    "\n",
    "            # if the key is not in the count table keys, it should put the key into the count table \n",
    "            # dictionary\n",
    "            if key not in result.keys():\n",
    "                result[key] = defaultdict(int)\n",
    "\n",
    "            # add one to the token count\n",
    "            result[key][token] += 1\n",
    "\n",
    "            # add the token to the history\n",
    "            history.append(token)\n",
    "   \n",
    "    # return the result\n",
    "    return result\n",
    "\n",
    "\n",
    "def most_likely_tags(corpus):\n",
    "    \n",
    "\n",
    "    bigram = count_ngrams(corpus, 1)\n",
    "\n",
    "    for tag in bigram:\n",
    "        max_value = 0\n",
    "        for next_tag in bigram[tag]:\n",
    "            count = bigram[tag][next_tag]\n",
    "            if (count > max_value):\n",
    "                max_value = count\n",
    "                max_tag = next_tag\n",
    "        print (tag, \"is followed the most by\", max_tag, \",\", max_value, \"times\")\n",
    "        \n",
    "most_likely_tags(ptb_tags)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex4-3\" style=\"color:red\">**Exercise 4-3**</a> **[1 point]** Find the most frequent verb on the PTB and on the Brown corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent verb of PTB is ' is ', 671 times\n",
      "The most frequent verb of PTB is ' is ', 10010 times\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "def extract_and_count_verbs(corpus):\n",
    "    \n",
    "    # intial list of verbs (will be a list of lists)\n",
    "    verbs = []\n",
    "    \n",
    "    # loop over the corpus\n",
    "    for sentence in corpus:\n",
    "        \n",
    "        # only keep the verbs \n",
    "        verbs.append([word for word, tag in sentence if tag == \"VERB\"])\n",
    "    \n",
    "    # count the tokens\n",
    "    count = count_tags(verbs)\n",
    "    \n",
    "    # return\n",
    "    return count\n",
    "\n",
    "# count the verbs in the corpora\n",
    "ptb_verbs = extract_and_count_verbs(ptb_corpus)\n",
    "brown_verbs = extract_and_count_verbs(brown_corpus)\n",
    "\n",
    "# get the key value combination of of the highest value in the dict\n",
    "most_frequent_ptb = max(ptb_verbs.items(), key=operator.itemgetter(1))\n",
    "most_frequent_brown = max(brown_verbs.items(), key=operator.itemgetter(1))\n",
    "\n",
    "\n",
    "print(\"The most frequent verb of PTB is '\", most_frequent_ptb[0], \"',\", most_frequent_ptb[1],\"times\")\n",
    "print(\"The most frequent verb of PTB is '\", most_frequent_brown[0], \"',\", most_frequent_brown[1],\"times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"hmm\"> Hidden Markov Models\n",
    "\n",
    "* How can we learn a PoS tagger given this dataset?\n",
    "\n",
    "The Hidden Markov Model **HMM** is a sequence model. A sequence model or sequence classifier is a\n",
    "model whose job is to assign a label or class to each unit in a sequence, thus mapping\n",
    "a sequence of observations to a sequence of labels.\n",
    "\n",
    "For example, in PoS the HMM will assing a tag to each word in a sentence: \n",
    "\n",
    "*Mr. Vinken is chairman of Elsevier N.V. the Dutch publishing group.*\n",
    "\n",
    "The output of a tagger would look like one of the annotated sentences form the Treeban corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Mr.', 'NOUN'), ('Vinken', 'NOUN'), ('is', 'VERB'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Elsevier', 'NOUN'), ('N.V.', 'NOUN'), (',', '.'), ('the', 'DET'), ('Dutch', 'NOUN'), ('publishing', 'VERB'), ('group', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(treebank_training[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining the **HMM**\n",
    "    \n",
    "We consider two phenomena:\n",
    "* Transition: we move from one \"state\" to another \"state\" where our state is the POS tag\n",
    "* Emission: with a certain \"state\" in mind, we generate a certain word\n",
    "\n",
    "This means that in the sentence above, for example, we generate\n",
    "1. From the state `BoS` (begin of sequence) we generate the state `NOUN`\n",
    "2. Then from `NOUN` we generate the word `Mr.`\n",
    "3. We then \"forget\" the word we just emitted and use the fact that our current state is `NOUN` to generate the next state, which is again a `NOUN`\n",
    "4. From where we then generate `Vinken`\n",
    "5. We proceed like that until we exaust both sequences\n",
    "\n",
    "\n",
    "Let us give names to things, let's model the current class with a random variable $C$ and let's use the random variable $C_{\\text{prev}}$ to model the previous category. For the word we will use the random variable $X$.\n",
    "Both $C$ and $C_{\\text{prev}}$ take on values in the enumeration of a tagset containing $t$ tags, that is, $\\{1, \\ldots, t\\}$. $X$ takes on values in the enumeration of a vocabulary containing $v$ words, that is, $\\{1, \\ldots, v\\}$.\n",
    "\n",
    "The **transition** distribution captures how our beliefs in a class vary as a function of the previous class. We will use Categorical distributions for that. In fact, for each possible previous class we get a Categorical distribution over the complete set of classes.\n",
    "\n",
    "\\begin{align}\n",
    "(1) \\qquad C \\mid C_{\\text{prev}}=p \\sim \\text{Cat}(\\lambda_1^{(p)}, \\ldots, \\lambda_t^{(p)})\n",
    "\\end{align}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex4-4\" style=\"color:red\">**Exercise 4-4**</a> **[1 point]** What is the probability value of  $P_{C|C_{\\text{prev}}}(c|p)$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex4-5\" style=\"color:red\">**Exercise 4-5**</a> **[2 points]**\n",
    "\n",
    "* **[1 point]** How many cpds do we need in order to represent all transition distributions?\n",
    "* **[1 point]** What's the representation cost of such a set of distributions? Use [big-O notation](https://en.wikipedia.org/wiki/Big_O_notation).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The **emission** distribution captures how our beliefs in a word vary as a function of the word's class. We will again use Categorical distributions for that. In fact, for each possible class, we get a Categorical distribution over the complete vocabulary.\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "(2) \\qquad X \\mid C=c \\sim \\text{Cat}(\\theta_1^{(c)}, \\ldots, \\theta_v^{(c)})\n",
    "\\end{align}\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex4-6\" style=\"color:red\">**Exercise 4-6**</a> **[1 point]** What's the probability value of $P_{X|C}(x|c)$?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex4-7\" style=\"color:red\">**Exercise 4-7**</a> **[2 points]** \n",
    "\n",
    "* **[1 point]** How many cpds do we need in order to represent all emission distributions? \n",
    "* **[1 point]** What's the representation cost of such a set of distributions? Use [big-O notation](https://en.wikipedia.org/wiki/Big_O_notation).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's turn to the joint distribution $P_{CX|C_{\\text{prev}}}$ over classes and words and let's focus on a single step where we can assume the previous class is already available. Then the model factorises as follows:\n",
    "\n",
    "\\begin{align}\n",
    "(3) \\qquad P_{CX|C_{\\text{prev}}}(x, c | c_{\\text{prev}}) &= P_{C|C_{\\text{prev}}}(c|c_{\\text{prev}}) P_{X|C}(x|c) \n",
    "\\end{align}\n",
    "\n",
    "It will turn out useful to know what is the marginal distribution $P_{X|C_{\\text{prev}}}$ over words given the previous class ---  where we have marginalised the current class out. \n",
    "\n",
    "\\begin{align}\n",
    "(4) \\qquad P_{X|C_{\\text{prev}}}(x| c_{\\text{prev}}) &= \\sum_{c=1}^t P_{CX|C_{\\text{prev}}}(x, c | c_{\\text{prev}}) \\\\\n",
    " &= \\sum_{c=1}^t P_{C|C_{\\text{prev}}}(c|c_{\\text{prev}}) \\times P_{X|C}(x|c) \n",
    "\\end{align}\n",
    "\n",
    "Note that Equation (4) starts by simply summing Equation (3) for all possible values of the current class, that is, for $c$ from 1 to $t$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our ultimate goal with the HMM is to assing a probability to a sentence $x_1^n$. For that we need to combine $n$ pairs $(c_i, x_i)$ and marginalise away their history $c_{i-1}$. Doing so yields the **joint probability**:\n",
    "\n",
    "\\begin{align}\n",
    "(5) \\qquad P_{X_1^nC_1^n|N}(x_1^n, c_1^n|n) &= P_{C|C_{\\text{prev}}}(c_1|c_0)P_{X|C}(x_1|c_1)P_{C|C_{\\text{prev}}}(c_2|c_1)P_{X|C}(x_2|c_1)\\cdots P_{C|C_{\\text{prev}}}(c_n|c_{n-1})P_{X|C}(x_n|c_n) \\\\\n",
    "    &= \\prod_{i=1}^n P(c_i|c_{i-1})P(x_i|c_i)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "We can get the **marginal probability** of a sentence by summing over all possible values of all possible class variables. This requires summing for each and every $c_i$ from 1 to $t$ as follows:\n",
    "\n",
    "\\begin{align}\n",
    "(6) \\qquad P_{S|N}(x_1^n|n) &= \\sum_{c_1=1}^t \\cdots \\sum_{c_n=1}^t P(c_1|c_0)P(x_1|c_1)P(c_2|c_1)P(x_2|c_1)\\cdots P(c_n|c_{n-1})P(x_n|c_n) \\\\\n",
    "&= \\sum_{c_1=1}^t \\cdots \\sum_{c_n=1}^t  \\prod_{i=1}^n P(c_i|c_{i-1})P(x_i|c_i)\n",
    "\\end{align}\n",
    "\n",
    "This looks pretty bad! If we have to enumerate all possible tag sequences, there would be just too many of them. That is, in the first sum, $c_1$ takes 1 of $t$ values, then for each of those values $c_2$ will take 1 of $t$ values, and so on. This leads to $t^n$ different tag sequences. An exponential number of them!!! We will never manage to enumerate them, compute their joint probabilities and then sum them up. \n",
    "\n",
    "Luckily, it turns out that in the HMM only two variables interact at a time, that is, $c_i$ interacts with $c_{i-1}$, then $c_i$ interacts with $x_i$, thus to characterise the distribution over $x_i$ we only need to know $c_{i-1}$ and $c_i$ --- which Equation (4) also shows. We can simplify Equation (6) by rearranging the sums and products as in Equation (7) below.\n",
    "\n",
    "\\begin{align}\n",
    "(7) \\qquad P_{S|N}(x_1^n|n) &= \\sum_{c_1=1}^t \\cdots \\sum_{c_n=1}^t  \\prod_{i=1}^n P(c_i|c_{i-1})P(x_i|c_i) \\\\\n",
    "&= \\prod_{i=1}^n \\sum_{c_{i-1}=1}^t \\sum_{c_i=1}^t P(c_i|c_{i-1})P(x_i|c_i)\n",
    "\\end{align}\n",
    "\n",
    "This is great news because it reveals that to marginalise the tag sequences we only need to scan over the sentence $i=1, \\ldots, n$ and then scan over the possible combinations of two tags from the tagset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex4-8\" style=\"color:red\">**Exercise 4-8**</a> **[3 points]** What is the algorithmic complexity of computing the marginal probability of a sentence in the HMM model?  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation (5) gives us a simple algorithm to assign joint probabilities to sequences of the kind $(c_1^n, x_1^n)$. \n",
    "Because we chose categorical distributions as in Equations (1) and (2), we can say that \n",
    "\n",
    "\\begin{align}\n",
    "P_{CX|C_{\\text{prev}}}(x, c | p) = \\lambda_{c}^{(p)} \\times \\theta_{x}^{(p)}\n",
    "\\end{align}\n",
    "\n",
    "and therefore Equation (5) is simply\n",
    "\n",
    "\\begin{align}\n",
    "(8) \\qquad P_{X_1^nC_1^n|N}(x_1^n, c_1^n|n) &= \\prod_{i=1}^n P(c_i|c_{i-1})P(x_i|c_i) \\\\\n",
    " &= \\prod_{i=1}^n  \\lambda_{c_i}^{(c_{i-1})} \\times \\theta_{x_i}^{(c_i)}\n",
    "\\end{align}\n",
    "\n",
    "Similarly, Equation (7) is great because it gives us an algorithm to assign probabilities to sentences. \n",
    "If we now use our parameters to rewrite Equation (7)  we get \n",
    "\n",
    "\\begin{align}\n",
    "(9) \\qquad P_{S|N}(x_1^n|n) &= \\prod_{i=1}^n \\sum_{c_{i-1}=1}^t \\sum_{c_i=1}^t P(c_i|c_{i-1})P(x_i|c_i) \\\\\n",
    "&= \\prod_{i=1}^n \\sum_{c_{i-1}=1}^t \\sum_{c_i=1}^t  \\lambda_{c_i}^{(c_{i-1})} \\times \\theta_{x_i}^{(c_i)}\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"mle\"> Maximum likelihood estimation for labelled data\n",
    "    \n",
    "As we know by now, MLE for Categorical CPDs is just a matter of counting and dividing.\n",
    "\n",
    "The MLE solution for the transition distribution is therefore\n",
    "\n",
    "\\begin{align}\n",
    "(10) \\qquad \\lambda_c^{(p)} = \\frac{\\text{count(p, c)}}{\\text{count}(p)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex4-9\" style=\"color:red\">**Exercise 4-9**</a> **[1 points]** What is the MLE solution for the emission distribution? Note: use the same style as in Equation (10), that is, state the parameter and the solution. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will **implement** the HMM, this means you will implement *transition distributions*, *emission distributions*, *Laplace smoothing*, *joint probability*, *marginal probability*, and an algorithm for making *predictions*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"imp\"> Implementation\n",
    "\n",
    "Here you will implement a language model based on an HMM POS tagger. \n",
    "\n",
    "<a name=\"ex4-10\" style=\"color:red\">**Exercise 4-10**</a> **[17 points]** \n",
    "\n",
    "You will need to complete the skeleton class below. Read it through before coding. Check the documentation for additional information. Document your steps (this will increase your chance of earning points in case you make mistakes along the way). \n",
    "\n",
    "* **[8 points]** Implement `estimate_model`: this should take an annotated corpus (such as PTB or Brown) and estimate the CPDs in the model using Laplace smoothing. \n",
    "* **[2 points]** Implement `transition_parameter`: see method's documentation\n",
    "* **[2 points]** Implement `emission_parameter`: see method's documentation\n",
    "* **[1 points]** Implement `joint_parameter`: see method's documentation\n",
    "* **[2 points]** Implement `log_joint`: see method's documentation\n",
    "* **[2 points]** Implement `log_marginal`: see method's documentation\n",
    "\n",
    "Show that you methods work by training a model and testing a few examples. This is an example \n",
    "\n",
    "```python\n",
    "treebank_hmm = HMMLM()\n",
    "treebank_hmm.estimate_model(treebank_training)\n",
    "print(treebank_hmm.joint_parameter('DET', 'NOUN', 'book'))\n",
    "sentence = [x for x, _ in treebank_dev[0]]\n",
    "tag_sequence = [c for _, c in treebank_dev[0]]\n",
    "print(' '.join(sentence))\n",
    "print(' '.join(tag_sequence))\n",
    "print(treebank_hmm.log_joint(sentence, tag_sequence))\n",
    "```\n",
    "\n",
    "for which we get\n",
    "\n",
    "```\n",
    "9.959527643028553e-05\n",
    "At Tokyo , the Nikkei index of 225 selected issues , which *T*-1 gained 132 points Tuesday , added 14.99 points to 35564.43 .\n",
    "ADP NOUN . DET NOUN NOUN ADP NUM VERB NOUN . DET X VERB NUM NOUN NOUN . VERB NUM NOUN PRT NUM .\n",
    "-193.71018537\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class HMMLM:\n",
    "    \"\"\"\n",
    "    This is our HMM language model class.\n",
    "    \n",
    "    It will be responsible for estimating parameters by MLE\n",
    "    as well as computing probabilities using the HMM.\n",
    "    \n",
    "    We will use Laplace smoothing by default (because we do not want to assign 0 probabilities).\n",
    "    \n",
    "    GUIDELINES:\n",
    "        - by convention we will use the string '-UNK-' for an unknown POS tag\n",
    "            - and '<unk>' for an unknown word\n",
    "        - don't forget that with Laplace smoothing the unknown symbols have to be in the support of distributions\n",
    "        - now you will have 2 types of distributions, so you should deal with unknown symbols for both of them\n",
    "        - we also need padding for sentences and tag sequences, by convention we will use \n",
    "            - '-BOS-' and '-EOS-' for padding tag sequences\n",
    "            - '<s>' and '</s>' for padding sentences\n",
    "        - do recall that '-BOS-' is **not** a valid tag\n",
    "            in other words we never *generate* '-BOS-' tags, we only pretend they occur at\n",
    "            the 0th position of the tag sequence in order to provide conditioning context\n",
    "            for the first actual tag\n",
    "        - similarly, '<s>' is not a valid word\n",
    "            in other words, we never *generate* '<s>' as a word\n",
    "            in fact '<s>' is optional as no emission event is based on it\n",
    "        - on the other hand, '-EOS-' is a valid tag\n",
    "            you should model it as the last event of a tag sequence\n",
    "        - similarly, '</s>' is a valid word\n",
    "            you should consider it as the last event of a sentence\n",
    "            Note: the last emission is always '-EOS-'' to '</s>', if you are suspecting this is unnecessary \n",
    "             because this emission will have probability 1.0, you are right about it! We still suggest modelling \n",
    "             it anyway, just to simplify your program (for example, if you model this dummy emission anyway, \n",
    "             there will be no need for special treatment of the last position).\n",
    "            \n",
    "    You can use whatever data structures you like for cpds\n",
    "        - we suggest python dict or collections.defaultdict\n",
    "            but you are free to experiment with list and/or np.array if you like\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, transition_alpha=1.0, emission_alpha=1.0):\n",
    "        self._vocab = set()\n",
    "        self._tagset = set()\n",
    "        self._emission_cpds = dict()\n",
    "        self._transition_cpds = dict()\n",
    "        self._transition_alpha = transition_alpha\n",
    "        self._emission_alpha = emission_alpha\n",
    "        \n",
    "    def tagset(self):\n",
    "        \"\"\"\n",
    "        Return the tagset: a set of all tags seen by the model (including '-UNK-').\n",
    "        \n",
    "        You can modify this if you judge necessary (for example, because you decided  to \n",
    "            use different datastructures, but do note that we provide you an implementation\n",
    "            of the Viterbi algorithm that expects this functionality).        \n",
    "        \"\"\"        \n",
    "        # the -BOS- tag is just something for internal representation\n",
    "        #  in case you have added it to the tagset, we are removing it here\n",
    "        #  as keeping it would be bad for algorithms such as Viterbi\n",
    "        # the -UNK- tag must be in the support (due to Laplace smoothing)\n",
    "        #  thus in case you forgot it, we are adding it now\n",
    "        return self._tagset - {'-BOS-'} | {'-UNK-'}\n",
    "    \n",
    "    def vocab(self):\n",
    "        \"\"\"\n",
    "        Return the vocabulary of words: all words seen by the model (including '<unk>').\n",
    "        \n",
    "        You can modify this if you judge necessary (for example, because you decided  to \n",
    "            use different datastructures, but do note that we provide you an implementation\n",
    "            of the Viterbi algorithm that expects this functionality).        \n",
    "        \"\"\"        \n",
    "        # the <s> token is just something for internal representation\n",
    "        #  in case you have added it to the vocabulary, we are removing it here\n",
    "        # the <unk> word must be in the support (due to Laplace smoothing)\n",
    "        #  thus in case you forgot it, we are adding it now\n",
    "        return self._vocab - {'<s>'} | {'<unk>'}\n",
    "        \n",
    "    def preprocess_sentence(self, sentence, bos=True, eos=True):\n",
    "        \"\"\"\n",
    "        Preprocess a sentence by lowercasing its words and possibly padding it.\n",
    "        \n",
    "        :param sentence: a list of tokens (each a string)\n",
    "        :param bos: if True you will get <s> at the beginning \n",
    "        :param eos: if True you will get </s> at the end\n",
    "        :returns: a list of tokens (lowercased strings)\n",
    "        \"\"\"\n",
    "        # lowercase\n",
    "        sentence = [x.lower() for x in sentence]\n",
    "        # optional padding\n",
    "        if bos: \n",
    "            sentence = ['<s>'] + sentence\n",
    "        if eos:\n",
    "            sentence = sentence + ['</s>']\n",
    "        return sentence\n",
    "        \n",
    "    def preprocess_tag_sequence(self, tag_sequence, bos=True, eos=True):\n",
    "        \"\"\"\n",
    "        Preprocess a tag sequence with optional padding.\n",
    "        \n",
    "        :param tag_sequence: a list of tags (each a string)\n",
    "        :param bos: if True you will get -BOS- at the beginning \n",
    "        :param eos: if True you will get -EOS- at the end\n",
    "        :returns: a list of tokens \n",
    "        \"\"\"\n",
    "        # optional padding\n",
    "        if bos:\n",
    "            tag_sequence = ['-BOS-'] + tag_sequence\n",
    "        if eos:\n",
    "            tag_sequence = tag_sequence + ['-EOS-']\n",
    "        return tag_sequence\n",
    "        \n",
    "    def estimate_model(self, treebank):\n",
    "        \"\"\"\n",
    "        TYPE YOUR SOLUTION\n",
    "        \n",
    "        :param treebank: a sequence of observations as provided by nltk\n",
    "            each observation is a list of pairs (x_i, c_i)    \n",
    "            and they have not yet been pre-processed \n",
    "        \n",
    "        Estimate the model parameters.\n",
    "        \n",
    "        This method does not have to return anything, it simply computes the necessary cpds.        \n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def transition_parameter(self, previous_tag, current_tag):\n",
    "        \"\"\"\n",
    "        TYPE YOUR SOLUTION\n",
    "        \n",
    "        This method returns the transition probability for tag given the previous tag.\n",
    "        \n",
    "        Tips: do not forget that we have a smoothed model, thus \n",
    "            - if the either tag was never seen, you should pretend it to be '-UNK-'\n",
    "        \n",
    "        :param previous_tag: the previous tag (str)\n",
    "        :param current_tag: the current tag (str)\n",
    "        :return: transition parameter\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def emission_parameter(self, tag, word):\n",
    "        \"\"\"\n",
    "        TYPE YOUR SOLUTION\n",
    "        \n",
    "        This method returns the emission probability for a word given a tag.\n",
    "        Tips: do not forget that we have a smoothed model, thus \n",
    "            - if the tag was never seen, you should pretend it to be '-UNK-'\n",
    "            - similarly, if the word was never seen, you shoud pretend it to be '<unk>'\n",
    "        \n",
    "        :param tag: the current tag (str)\n",
    "        :param word: the current word (str)\n",
    "        :return: the emission probability\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def joint_parameter(self, previous_tag, current_tag, word):\n",
    "        \"\"\"\n",
    "        TYPE YOUR SOLUTION\n",
    "        \n",
    "        This method returns the joint probability of (current tag, word) given the previous tag\n",
    "            according to Equation (3)\n",
    "            \n",
    "        :param previous_tag: the previous tag (str)\n",
    "        :param current_tag: the current tag (str)\n",
    "        :param word: the current word (str)\n",
    "        :returns: P(word, current_tag|previous_tag)\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def marginal_x_given_cprev(self, previous_tag, word):\n",
    "        \"\"\"\n",
    "        Return P(x|prev) as defined in Equation (4) by marginalising current tag.\n",
    "        \n",
    "        :param previous_tag: the previous tag (str)\n",
    "        :param word: the current word (str)\n",
    "        \"\"\"\n",
    "        return np.sum([self.joint_parameter(previous_tag, c, word) for c in self._tagset])\n",
    "    \n",
    "    def log_joint(self, sentence, tag_sequence):\n",
    "        \"\"\"\n",
    "        TYPE YOUR SOLUTION\n",
    "        \n",
    "        Implement the logarithm of the joint probability over a sentence and tag sequence as in Equation (8)\n",
    "        \n",
    "        :param sentence: a sequence of words (each a string) not yet preprocessed\n",
    "        :param tag_sequence: a sequence of tags (eac a string) not yet preprocessed\n",
    "        :returns: log P(x_1^n, c_1^n|n) as defined in Equation (8)\n",
    "        \"\"\" \n",
    "        pass\n",
    "    \n",
    "    def log_marginal(self, sentence):\n",
    "        \"\"\"\n",
    "        TYPE YOUR SOLUTION\n",
    "        \n",
    "        Implement the logarithm of the marginal probability of a sentence as in Equation (9)\n",
    "            by marginalisation of all possible tag sequences. \n",
    "            \n",
    "        :param sentence: a sequence of words (each a string) not yet preprocessed\n",
    "        :returns: log P(x_1^m|n) as defined in Equation (9)\n",
    "        \"\"\"        \n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"eval\"> Evaluation\n",
    "    \n",
    "We can evaluate our models by the computing the log-perplexity, like with the LM or by comparing the predictions of the trained model with an annotated test set.\n",
    "\n",
    "\n",
    "## <a name=\"ppl\"> Perplexity\n",
    "\n",
    "Perplexity of a model on a test set is the inverse probability of the test set, normalized\n",
    "by the number of words. Perplexity is a notion of average branching factor, thus a model with low perplexity can be thought of as a *less confused*. That is, each time it introduces a word given some history it picks from a reduced subset of the entire vocabulary (in other words, it is more certain of how to continue). \n",
    "\n",
    "If a dataset contains $t$ tokens where $t = \\sum_{k=1}^m n_k$, then the perplexity of the dataset is\n",
    "\n",
    "\\begin{equation}\n",
    "(11) \\qquad \\text{PP}(\\mathcal T) = \\left( \\prod_{k=1}^m P_{S|N}(\\langle x_1^{(k)}, \\ldots, x_{n_k}^{(k)} \\rangle|n_k; \\boldsymbol \\theta) \\right)^{-1/t}\n",
    "\\end{equation}\n",
    "\n",
    "where we have already discarded the length distribution (since it's held constant across models). And the probability of the sentence requires marginalising tag sequences, as shown in Equation (9).\n",
    "\n",
    "It's again convenient to use log and define log-perplexity\n",
    "\n",
    "\\begin{equation}\n",
    "(12) \\qquad \\log \\text{PP}(\\mathcal T) = - \\frac{1}{t} \\sum_{k=1}^m \\log P_{S|N}(\\langle x_1^{(k)}, \\ldots, x_{n_k}^{(k)} \\rangle|n_k; \\boldsymbol \\theta) \n",
    "\\end{equation}\n",
    "\n",
    "You can compare models in terms of the log-perplexity they assign to the same test data. The lower the perplexity, the better the model is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex4-11\" style=\"color:red\">**Exercise 4-11**</a> **[4 points]** Implement `log_perplexity` below. Train models of PTB and Brown and test both of them on their respective test sets as well as on each other's test set. Report all results.\n",
    "\n",
    "To help you have an idea whether you implemented it right, this is an excerpt of what we got with our implementation\n",
    "\n",
    "```python\n",
    "dev_sentences = extract_sentences(treebank_dev)\n",
    "log_perplexity(dev_sentences, treebank_hmm)\n",
    "```\n",
    "\n",
    "```\n",
    "101.80708039918399\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, this is how you load the Brown corpus. Let's use the last 1000 sentences for test. You can reduce the size of the training set if your computer cannot handle what we suggest below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57340 56000 340 1000\n"
     ]
    }
   ],
   "source": [
    "# load the Brown corpus wiht the universal tag set\n",
    "brown_sentences = nltk.corpus.brown.tagged_sents(tagset='universal')\n",
    "\n",
    "# we split our corpus on training, dev and test\n",
    "brown_training = list(brown_sentences[:56000]) \n",
    "brown_dev = list(brown_sentences[56000:56340])\n",
    "brown_test = list(brown_sentences[56340:])\n",
    "print(len(brown_sentences), len(brown_training), len(brown_dev), len(brown_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_perplexity(sentences, hmm):\n",
    "    \"\"\"\n",
    "    TYPE YOUR SOLUTION\n",
    "    \n",
    "    For a dataset of sentences (each sentence is a list of words)\n",
    "        and an instance of the HMMLM class\n",
    "        return the log perplexity as defined in Equation (12)\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"viterbi\"> Viterbi\n",
    "\n",
    "\n",
    "The Viterbi algorithm is used to search through the space of possible tag sequences and find the one that score highest.\n",
    "\n",
    "The space of tag sequences is very large, consider that we can select any of $t$ tags for each position, thus by simple counting, we are left with $t^n$ possible sequences. Searching through an exponential space is intractable in general. \n",
    "\n",
    "Not by chance we designed the HMM with certain conditional independence assumptions. In fact, in our HMM model the probability of each word observation really only depends on two decisions, namely, its tag and its preceding tag. We can use that to derive a *tractable* dynamic program. \n",
    "\n",
    "[Dynamic programming](https://en.wikipedia.org/wiki/Dynamic_programming) is a kind of *divide and conquer* strategy. We have to identify sub-problems that we can solve efficiently and maintain a memory of partial solutions that we use to build the final one.\n",
    "\n",
    "For the Viterbi algorithm we need to solve the problem \"what is the best probability so far\". We will approach this problem with a recursion that computes the probability of the best path of a certain length.\n",
    "We will use the recursive function $\\alpha(i, j)$ which returns the best probability for a path of length $i$ assuming that its last tag is $C_i=j$. For efficiency in our implementation we will map tags to integers.\n",
    "\n",
    "The Viterbi recursion is pretty simple, if a path has length $0$ we will assume it has probability $1$. If a path has length $i$ (more than $0$), we reckon that its maximum probability is based on the maximum probability assigned to paths of size $i-1$, where we extend those paths with one steps and check which step yields the best probability.\n",
    "\n",
    "The Viterbi recursion is formalised below:\n",
    "\n",
    "\\begin{align}\n",
    "(13)\\qquad \\alpha(i, j) &= \n",
    "\\begin{cases}\n",
    "1 & \\text{if }i = 0 \\\\\n",
    "\\max_{p \\in \\{1, \\ldots, t\\}} \\alpha(i-1, p) \\times \\lambda_{j}^{p} \\times \\theta_{x_i}^{j} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "Note that in the second line, we look for paths of size $i-1$ (which my have ended in one of $t$ possible previous tags) and we try to extend them with tag $j$. The probability of each hypothetical extension is the probability of the path we mean to extend $\\alpha(i-1, p)$ times the joint parameters associated with the extension. That is, continuing from previous class $C_{i-1}=p$ with current class $C_i=j$ (that is a transition parameter), and generating the current word $X_i=x_i$ from the current class $C_i=j$ ( that is an emission parameter).\n",
    "\n",
    "Note that the recursive function takes 2 inputs, $i$ which ranges from 1 to $n$, and $j$ which ranges from 1 to $t$. Thus we need to make at most $nt$ calls to this function. But also note that each time we call it, we have to solve (in the second line) a maximisation over $t$ possible assignments of the previous tag. Therefore the overall complexity of the algorithm is $O(nt^2)$. That's quite an improvement from $O(t^n)$, isn't it?\n",
    "\n",
    "Of course, crucial to the success of the algorithm is that we use [memoization](https://en.wikipedia.org/wiki/Memoization), a technique by which we store partial solutions and never recompute things.\n",
    "\n",
    "We provide you with a **recursive** implementation of the Viterbi algorithm which uses the interface of the HMMLM class that you designed above. You can use this implementation if you want to experiment with PTB and Brown corpus in the exercises below. Read the implementation carefully to learn from it. We have provided extensive documentation.\n",
    "One important note is that we compute everything in log space, that's to rely on better numerical properties of log probabilities.\n",
    "\n",
    "The Viterbi recursion gives us a way to compute the *probability* of the best path. To find the actual best path we just need to traverse the table of $\\alpha(i, j)$ values looking for the decision (tag) that is best at each point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viterbi_recursion(sentence, hmm):\n",
    "    \"\"\"\n",
    "    Computes the best possible tag sequence for a given input\n",
    "    and also returns it log probability.\n",
    "    \n",
    "    This implementation uses recursion.\n",
    "    \n",
    "    :returns: tag sequence, log probability\n",
    "    \"\"\"\n",
    "    # here we pad the sentence with </s> only\n",
    "    sentence = hmm.preprocess_sentence(sentence, bos=False, eos=True)\n",
    "    # this is the length (but recall that padding added 1 token) \n",
    "    n = len(sentence)\n",
    "    # this is the complete tagset, which for convenience we will turn into a list\n",
    "    tagset = list(hmm.tagset())\n",
    "    t = len(tagset)\n",
    "    # We need a table to store log alpha(i, j) values\n",
    "    # - where i is an integer from 0 to n-1 which refers to a position in the list `sentence`\n",
    "    #   i.e. sentence[i]\n",
    "    # - and j is an integer from 0 to t-1 that refers to a tag in the list `tagset` \n",
    "    #   i.e. tagset[j] \n",
    "    # - together (i, j) means that we are setting `C_i = tagset[j]`    \n",
    "    # - we will be exploring the space of possible tags per position\n",
    "    #   thus our table has as many as n * t cells\n",
    "    # - Recall that the value \\log \\alpha(i, j)\n",
    "    #   corresponds to the log probability value of the best\n",
    "    #   path (C_1, ..., C_i) such that C_i = j\n",
    "    #   in other words the log probability of the best sequence up to the ith token where C_i = j\n",
    "    # At the beginning path probabilities have not been computed, we use a probability of 0 to indicate that\n",
    "    #  as we will be computing log probabilities, we use -inf instead\n",
    "    #  numpy arrays are very handy and we can actually use the quantity -inf\n",
    "    log_alpha_table = np.full([n, t], -float('inf'))\n",
    "    # In a best path algorithm we are interested in two things\n",
    "    #  the best score (or best log probability)\n",
    "    #  as well as the path that corresponds to the best score\n",
    "    # We compute the best score by moving i forward from 0 to n-1 computing the maximum value \n",
    "    #  and we traverse the table backwards following the path that led to the maximum\n",
    "    #  thus we create a table of \"back pointers\"\n",
    "    #  this is an integer for each cell (i, j) that tells us which tag `p` for position `i - 1`\n",
    "    #   leads to the score stored in `log_alpha_table[i, j]`\n",
    "    back_pointer_table = np.full([n, t], -1, dtype=int)\n",
    "\n",
    "    # Here we define the log alpha recursion\n",
    "    def log_alpha(i, j):\n",
    "        \"\"\"\n",
    "        This function returns\n",
    "                max_{c_1, ..., c_i=j} log P(c_1, ..., c_i=j) \n",
    "            where i is a (0-based) position in `sentence`\n",
    "            and j is a (0-based) position in `tagset`\n",
    "        \"\"\"\n",
    "        if i == 0:  # we do not need to tag the 0th position and it should not affect the probability\n",
    "            return 0.  # np.log(1)\n",
    "        # When we implement dynamic programs, we like to re-use computations already made\n",
    "        # thus first of all we test if we have already computed a value for this cell\n",
    "        # if so, it will not have a zero probability (-inf in log space)\n",
    "        if log_alpha_table[i, j] != -float('inf'):  \n",
    "            # then we can simply return it\n",
    "            return log_alpha_table[i, j]\n",
    "        # At this point we know we have not yet computed a score for this path\n",
    "        #  thus we proceed to compute it\n",
    "        # We will have to figure out the log prob of the best prefix\n",
    "        #  and which tag best continues from it\n",
    "        # There are exactly t classes that may tag this position\n",
    "        #  thus we just go over the tagset trying one at a time\n",
    "        #  and memorise the score we would have if we would select them\n",
    "        path_max_log_prob = np.full(t, -float('inf'))\n",
    "        for p in range(t):\n",
    "            # this is the essential part of the recursion\n",
    "            # we ask for the best score associated with the previous position \n",
    "            #  had it been tagged with p\n",
    "            #  and we incorporate the probability of C_i = tagset[j] given that C_{i-1} = tagset[p]\n",
    "            #   as well as the probability of X_i = sentence[i] given that C_i = tagset[j]\n",
    "            path_max_log_prob[p] = log_alpha(i - 1, p) + np.log(hmm.joint_parameter(tagset[p], tagset[j], sentence[i]))\n",
    "        # From all possibilities, we are only interested in the best\n",
    "        log_alpha_table[i, j] = np.max(path_max_log_prob)\n",
    "        # and we also want to store a pointer to the best\n",
    "        back_pointer_table[i, j] = np.argmax(path_max_log_prob)\n",
    "        return log_alpha_table[i, j]\n",
    "    \n",
    "    # Let's get the index associated with -EOS-\n",
    "    #  which is the tag for the </s> symbol in sentence[-1]\n",
    "    eos_index = tagset.index('-EOS-')\n",
    "    # We want the last word in the sentence (</s>) to have the tag -EOS-\n",
    "    #  thus we ask \"what's the probability of the best path that ends in -EOS-?\"\n",
    "    max_log_prob = log_alpha(n - 1, eos_index)\n",
    "    \n",
    "    # Here we retrieve the backpointers for the best analysis\n",
    "    #  the best analisys has n tags\n",
    "    bwd_argmax = [None] * n\n",
    "    #  the last tag is the -EOS- symbol\n",
    "    bwd_argmax[-1] = eos_index\n",
    "    # Here we maintain the \"current tag\" c_i\n",
    "    c_i = eos_index\n",
    "    for i in range(n - 1, 0, -1):  # we go backwards from c_{n-1} to c_1\n",
    "        # and set the value of c_{i-1} for the current c_i\n",
    "        bwd_argmax[i - 1] = back_pointer_table[i, c_i]\n",
    "        # we need, of course, to update c_i\n",
    "        c_i = bwd_argmax[i - 1]\n",
    "    \n",
    "    # Here we translate from ids back to actual tags (strings)\n",
    "    #  we leave the -EOS- symbol out, since it was just a convenience \n",
    "    #  and return both the tag sequence and the total log probability\n",
    "    return [tagset[c] for c in bwd_argmax[:-1]], max_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PRON', 'VERB', 'PRON', 'VERB', 'DET', 'NOUN', '.'] -42.8173715682\n"
     ]
    }
   ],
   "source": [
    "# let's tag a sentence\n",
    "viterbi_path, viterbi_log_prob = viterbi_recursion(['i', 'wish', 'i', 'had', 'a', 'book', '.'], treebank_hmm)\n",
    "print(viterbi_path, viterbi_log_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  <a name=\"acc\"> Accuracy\n",
    "\n",
    "We can evaluate the performance of our tagger by comparing its Viterbi predictions to human annotation. \n",
    "\n",
    "For this, we will use the gold standard test data (e.g. treebank_test). Evaluation metrics compute a score for a given model (e.g. our HMM tagger) by comparing the predicted labels that the model generated with the test data againts the gold standard annotation.\n",
    "\n",
    "The **Accuracy** metric computes the percentage of instances in the test data that our tagger labeled correctly.\n",
    "If we have a dataset of $m$ labelled sequences\n",
    "\\begin{equation}\n",
    "\\left( \\langle x_1^{(k)}, \\ldots, x_{n_k}^{(k)}\\rangle, \\langle \\star_1^{(k)}, \\ldots, \\star_{n_k}^{(k)}\\rangle \\right)_{k=1}^m\n",
    "\\end{equation}\n",
    "we can produce all Viterbi predictions\n",
    "\\begin{equation}\n",
    "\\left( \\langle x_1^{(k)}, \\ldots, x_{n_k}^{(k)}\\rangle, \\langle c_1^{(k)}, \\ldots, c_{n_k}^{(k)}\\rangle \\right)_{k=1}^m\n",
    "\\end{equation}\n",
    "\n",
    "and compute\n",
    "\n",
    "\\begin{align}\n",
    "(14)\\qquad \\text{accuracy} &= \\frac{\\sum_{k=1}^m \\sum_{i=1}^{n_k} [c_i^{(k)} = \\star_i^{(k)}]}{\\sum_{k=1}^m n_k}\\\\\n",
    "&= \\frac{\\text{number of correct predictions}}{\\text{total tokens}} \n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex4-12\" style=\"color:red\">**Exercise 4-12**</a> **[5 points]** \n",
    "\n",
    "* **[1 point]** Implement the accuracy function.\n",
    "* **[0.5 point]** Compute accuracy on PTB's test set using a PTB-trained model\n",
    "* **[0.5 point]** Compute accuracy on PTB's test set using a Brown-trained model\n",
    "* **[0.5 point]** Compute accuracy on Brown's test set using a PTB-trained model\n",
    "* **[0.5 point]** Compute accuracy on Brown's test set using a Brown-trained model\n",
    "* **[2 points]** Show examples of correctly and incorrectly predicted sequences and discuss what might have gone wrong in the incorrect cases.\n",
    "\n",
    "For this exercise you can use the Viterbi implementation we provided (or your own -- its up to you). With our own implementation we got $0.8446$ accuracy for PTB-training and PTB-test, and $0.8960$ for Brown-training and Brown-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(gold_sequences, pred_sequences):\n",
    "    \"\"\"\n",
    "    Return percentage of instances in the test data that our tagger labeled correctly.\n",
    "    \n",
    "    :param gold_sequences: a list of tag sequences that can be assumed to be correct\n",
    "    :param pred_sequences: a list of tag sequences predicted by Viterbi    \n",
    "    \"\"\"\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
